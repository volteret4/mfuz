#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import sys
import urllib.parse
import os
import sqlite3
import time
import json
import argparse
import datetime
from urllib.parse import urlparse, quote_plus
import traceback
import re
import random


# Importamos aclarar_contenido.py para usar su funci√≥n
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(__file__))))
import tools.aclarar_contenido


def verificar_y_crear_tablas(conn, cursor):
    """
    Verifica y crea todas las tablas necesarias si no existen
    
    Args:
        conn: Conexi√≥n a la base de datos
        cursor: Cursor de la base de datos
        
    Returns:
        bool: True si todo se cre√≥ correctamente
    """
    try:
        # Crear tabla feeds b√°sica si no existe
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS feeds (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                entity_type TEXT NOT NULL,
                entity_id INTEGER NOT NULL,
                feed_name TEXT NOT NULL,
                post_title TEXT NOT NULL,
                post_url TEXT NOT NULL,
                post_date TIMESTAMP,
                content TEXT,
                added_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        print("‚úì Tabla feeds verificada/creada")
        
        # Crear columna origen si no existe
        crear_columna_origen_si_no_existe(cursor)
        
        # Crear tabla album_metacritic si no existe
        crear_tabla_metacritic(cursor)
        
        # Crear tabla album_aoty si no existe
        crear_tabla_aoty(cursor)
        
        # Commit de todas las creaciones
        conn.commit()
        print("‚úì Todas las tablas verificadas/creadas exitosamente")
        return True
        
    except sqlite3.Error as e:
        print(f"Error al verificar/crear tablas: {e}")
        conn.rollback()
        return False



def buscar_album_en_db(conn, cursor, album_id, album_name, artist_name, content_service, archivo_errores=None):
    """
    Busca un √°lbum en AnyDecentMusic y guarda las rese√±as encontradas
    """
    # Asegurar que las tablas existen antes de procesar
    verificar_y_crear_tablas(conn, cursor)
    
    # Resto de la funci√≥n original...
    termino_busqueda = artist_name
    url_busqueda = f"http://www.anydecentmusic.com/search-results.aspx?search={urllib.parse.quote(termino_busqueda)}"
    
    print(f"Buscando √°lbum: {album_name}")
    print(f"Artista: {artist_name}")
    print(f"URL de b√∫squeda: {url_busqueda}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        respuesta = requests.get(url_busqueda, headers=headers)
        respuesta.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Error al realizar la petici√≥n: {e}")
        return 0
    
    soup = BeautifulSoup(respuesta.text, 'html.parser')
    resultados = soup.select('form > div > div > div > ul > li > div')
    
    if not resultados:
        print("No se encontraron resultados para la b√∫squeda.")
        return 0
    
    print(f"Se encontraron {len(resultados)} resultados.")
    
    album_encontrado = False
    enlaces_guardados = 0
    
    for idx, resultado in enumerate(resultados, 1):
        artista_elemento = resultado.select_one('a:nth-of-type(2) > h2')
        album_elemento = resultado.select_one('a:nth-of-type(3) > h3')
        
        if artista_elemento and album_elemento:
            nombre_artista = artista_elemento.text.strip()
            nombre_album = album_elemento.text.strip()
            
            print(f"Resultado {idx}: {nombre_artista} - {nombre_album}")
            
            if (artist_name.lower() in nombre_artista.lower() and 
                album_name.lower() in nombre_album.lower()):
                print(f"¬°Coincidencia encontrada! Artista: {nombre_artista}, √Ålbum: {nombre_album}")
                album_encontrado = True
                
                album_url_elemento = resultado.select_one('a:nth-of-type(3)')
                if album_url_elemento and album_url_elemento.has_attr('href'):
                    album_url = album_url_elemento['href']
                    album_url_completa = f"http://www.anydecentmusic.com/{album_url}"
                    print(f"URL del √°lbum: {album_url_completa}")
                    
                    enlaces_validos, enlaces_error = extraer_enlaces_album(album_url_completa)
                    
                    if archivo_errores and enlaces_error:
                        guardar_errores_enlace(archivo_errores, nombre_artista, nombre_album, enlaces_error)
                    
                    if enlaces_validos:
                        print(f"Se encontraron {len(enlaces_validos)} enlaces de rese√±as.")
                        
                        for enlace in enlaces_validos:
                            url = enlace['url']
                            estado = enlace['estado']
                            
                            print(f"Procesando enlace: {url}")
                            
                            feed_name = extraer_dominio(url)
                            post_title = obtener_titulo_pagina(url)
                            contenido = extraer_contenido_con_aclarar(url, content_service)
                            
                            if contenido:
                                post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                guardado = guardar_feed(conn, cursor, 'album', album_id, feed_name, 
                                                      post_title, url, post_date, contenido)
                                
                                if guardado:
                                    enlaces_guardados += 1
                                    print(f"Rese√±a guardada: {feed_name} - {post_title}")
                            else:
                                print(f"No se pudo extraer contenido de {url}")
                    else:
                        print("No se encontraron enlaces de rese√±as v√°lidos.")
    
    if not album_encontrado:
        print(f"No se encontr√≥ el √°lbum '{album_name}' para el artista '{artist_name}'.")
    
    return enlaces_guardados



def extraer_enlaces_album(url_album):
    """
    Accede a la p√°gina del √°lbum y extrae todos los enlaces "Read Review".
    Verifica tambi√©n si los enlaces est√°n activos, retornan 404 o redireccionan.
    Tambi√©n registra errores no est√°ndar (diferentes de 404 o 403).
    
    Args:
        url_album (str): URL de la p√°gina del √°lbum
        
    Returns:
        tuple: (enlaces_validos, enlaces_error) donde ambos son listas de enlaces
    """
    print(f"Accediendo a la p√°gina del √°lbum: {url_album}")
    
    # Realizar la petici√≥n HTTP
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        respuesta = requests.get(url_album, headers=headers)
        respuesta.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Error al acceder a la p√°gina del √°lbum: {e}")
        return [], []
    
    # Parsear el HTML
    soup = BeautifulSoup(respuesta.text, 'html.parser')
    
    # Buscar todos los elementos de la lista que contienen los enlaces
    elementos_lista = soup.select('form > div > div > div > ol > li')
    
    enlaces_validos = []
    enlaces_error = []
    total_enlaces = 0
    total_read_review = 0
    
    for idx, elemento in enumerate(elementos_lista, 1):
        # Buscar el enlace dentro de cada elemento de la lista
        enlace_elemento = elemento.select_one('p > a')
        if enlace_elemento and enlace_elemento.has_attr('href'):
            total_enlaces += 1
            url = enlace_elemento['href']
            texto = enlace_elemento.text.strip()
            
            # Solo procesar los enlaces "Read Review"
            if "Read Review" in texto:
                total_read_review += 1
                
                # CORRECCI√ìN: Verificar y construir la URL completa correctamente
                if url.startswith('http://') or url.startswith('https://'):
                    url_completa = url
                elif url.startswith('/'):
                    url_completa = f"http://www.anydecentmusic.com{url}"
                else:
                    # Si no tiene protocolo ni empieza con /, agregar el prefijo completo
                    url_completa = f"http://www.anydecentmusic.com/{url}"
                
                # VALIDACI√ìN ADICIONAL: Verificar que la URL sea v√°lida antes de hacer la petici√≥n
                try:
                    parsed_url = urllib.parse.urlparse(url_completa)
                    if not parsed_url.scheme or not parsed_url.netloc:
                        print(f"URL inv√°lida detectada: {url_completa}")
                        estado = "URL inv√°lida"
                        enlaces_error.append({
                            'numero': idx,
                            'texto': texto,
                            'url': url_completa,
                            'estado': estado,
                            'url_original': url
                        })
                        continue
                except Exception as url_parse_error:
                    print(f"Error al parsear URL: {url_completa} - {url_parse_error}")
                    estado = f"Error de parsing URL: {str(url_parse_error)}"
                    enlaces_error.append({
                        'numero': idx,
                        'texto': texto,
                        'url': url_completa,
                        'estado': estado,
                        'url_original': url
                    })
                    continue
                
                try:
                    # Configurar para seguir redirecciones pero guardar la URL final
                    resp = requests.head(
                        url_completa, 
                        headers=headers, 
                        allow_redirects=True, 
                        timeout=10
                    )
                    
                    # Determinar el estado del enlace
                    if resp.status_code == 200:
                        estado = "Activo"
                        # Verificar si hubo redirecci√≥n
                        if resp.url != url_completa:
                            estado = f"Redirigido a: {resp.url}"
                            url_completa = resp.url  # Actualizar URL a la redirecci√≥n
                        
                        enlaces_validos.append({
                            'numero': idx,
                            'texto': texto,
                            'url': url_completa,
                            'estado': estado,
                            'url_original': url
                        })
                    elif resp.status_code in [404, 403]:
                        estado = f"Error {resp.status_code} - No disponible"
                        # No registramos estos errores comunes
                    else:
                        estado = f"Error HTTP: {resp.status_code}"
                        enlaces_error.append({
                            'numero': idx,
                            'texto': texto,
                            'url': url_completa,
                            'estado': estado,
                            'codigo': resp.status_code,
                            'url_original': url
                        })
                        
                except requests.exceptions.RequestException as e:
                    estado = f"Error al verificar: {str(e)}"
                    enlaces_error.append({
                        'numero': idx,
                        'texto': texto,
                        'url': url_completa,
                        'estado': estado,
                        'excepcion': str(e),
                        'url_original': url
                    })
                
                print(f"Enlace {idx}: {texto} - {url_completa} -> {estado}")
    
    print(f"Total de enlaces: {total_enlaces}")
    print(f"Total de enlaces 'Read Review': {total_read_review}")
    print(f"Enlaces v√°lidos encontrados: {len(enlaces_validos)}")
    print(f"Enlaces con errores no est√°ndar: {len(enlaces_error)}")
    
    return enlaces_validos, enlaces_error


# Tambi√©n a√±adir esta funci√≥n de utilidad para limpiar URLs problem√°ticas
def limpiar_url(url):
    """
    Limpia y valida una URL antes de usarla
    
    Args:
        url (str): URL a limpiar
        
    Returns:
        str or None: URL limpia y v√°lida, o None si no es v√°lida
    """
    try:
        # Decodificar URL encoding si es necesario
        url_decoded = urllib.parse.unquote(url)
        
        # Validar que no tenga caracteres extra√±os
        parsed = urllib.parse.urlparse(url_decoded)
        
        # Verificar que tenga esquema y netloc v√°lidos
        if not parsed.scheme:
            return None
        if not parsed.netloc:
            return None
            
        # Reconstruir la URL limpia
        url_limpia = urllib.parse.urlunparse(parsed)
        
        return url_limpia
    except Exception as e:
        print(f"Error limpiando URL {url}: {e}")
        return None

def guardar_errores_enlace(archivo_errores, artista, album, enlaces_error):
    """
    Guarda los enlaces con errores no est√°ndar en un archivo
    
    Args:
        archivo_errores (str): Ruta al archivo donde guardar los errores
        artista (str): Nombre del artista
        album (str): Nombre del √°lbum
        enlaces_error (list): Lista de enlaces con errores
        
    Returns:
        bool: True si se guard√≥ correctamente, False en caso contrario
    """
    if not enlaces_error:
        return True
        
    try:
        # Crear el directorio si no existe
        directorio = os.path.dirname(archivo_errores)
        if directorio and not os.path.exists(directorio):
            os.makedirs(directorio)
            
        # Abrir en modo append para no sobrescribir errores anteriores
        with open(archivo_errores, 'a', encoding='utf-8') as f:
            f.write(f"\n{'='*50}\n")
            f.write(f"Artista: {artista}\n")
            f.write(f"√Ålbum: {album}\n")
            f.write(f"Fecha: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*50}\n\n")
            
            for enlace in enlaces_error:
                f.write(f"URL: {enlace['url']}\n")
                f.write(f"Texto: {enlace['texto']}\n")
                f.write(f"Estado: {enlace['estado']}\n")
                
                # A√±adir detalles adicionales si est√°n disponibles
                if 'codigo' in enlace:
                    f.write(f"C√≥digo HTTP: {enlace['codigo']}\n")
                if 'excepcion' in enlace:
                    f.write(f"Excepci√≥n: {enlace['excepcion']}\n")
                
                f.write("\n")
                
        print(f"Errores guardados en {archivo_errores}")
        return True
    except Exception as e:
        print(f"Error al guardar errores en archivo: {e}")
        return False


def conectar_db(db_path):
    """
    Establece conexi√≥n con la base de datos SQLite
    
    Args:
        db_path (str): Ruta al archivo de la base de datos
        
    Returns:
        tuple: Conexi√≥n y cursor a la base de datos
    """
    try:
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row  # Para acceder a las columnas por nombre
        cursor = conn.cursor()
        return conn, cursor
    except sqlite3.Error as e:
        print(f"Error al conectar a la base de datos: {e}")
        sys.exit(1)

def obtener_albums(cursor):
    """
    Obtiene todos los √°lbumes de la base de datos
    
    Args:
        cursor: Cursor de la base de datos
        
    Returns:
        list: Lista de diccionarios con informaci√≥n de los √°lbumes
    """
    try:
        # Consulta para obtener √°lbumes con artista principal
        cursor.execute("""
            SELECT albums.id, albums.name, artists.name as artist_name 
            FROM albums 
            JOIN artists ON albums.artist_id = artists.id
            ORDER BY albums.id
        """)
        
        albums = []
        for row in cursor.fetchall():
            albums.append({
                'id': row['id'],
                'name': row['name'],
                'artist': row['artist_name']
            })
        
        print(f"Se encontraron {len(albums)} √°lbumes en la base de datos")
        return albums
    except sqlite3.Error as e:
        print(f"Error al obtener √°lbumes: {e}")
        return []



def extraer_dominio(url):
    """
    Extrae el nombre del dominio de una URL
    
    Args:
        url (str): URL completa
        
    Returns:
        str: Nombre del dominio (ej: 'pitchfork.com')
    """
    try:
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        # Eliminar 'www.' si existe
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain
    except Exception as e:
        print(f"Error al extraer dominio de {url}: {e}")
        return "unknown_domain"

def obtener_titulo_pagina(url):
    """
    Intenta obtener el t√≠tulo de una p√°gina web
    
    Args:
        url (str): URL de la p√°gina
        
    Returns:
        str: T√≠tulo de la p√°gina o una cadena predeterminada
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        title_tag = soup.find('title')
        
        if title_tag and title_tag.text:
            # Limpiar el t√≠tulo eliminando saltos de l√≠nea y espacios excesivos
            title = re.sub(r'\s+', ' ', title_tag.text).strip()
            return title
        
        # Si no hay t√≠tulo, extraer el dominio y crear un t√≠tulo gen√©rico
        domain = extraer_dominio(url)
        return f"Review on {domain}"
    except Exception as e:
        print(f"Error al obtener t√≠tulo de {url}: {e}")
        domain = extraer_dominio(url)
        return f"Review on {domain}"

def guardar_feed(conn, cursor, entity_type, entity_id, feed_name, post_title, post_url, post_date, content):
    """
    Guarda la informaci√≥n de una rese√±a en la tabla feeds
    
    Args:
        conn: Conexi√≥n a la base de datos
        cursor: Cursor de la base de datos
        entity_type (str): Tipo de entidad ('album')
        entity_id (int): ID de la entidad
        feed_name (str): Nombre del feed (dominio)
        post_title (str): T√≠tulo del post
        post_url (str): URL del post
        post_date (str): Fecha del post (o None)
        content (str): Contenido del post
        
    Returns:
        bool: True si se guard√≥ correctamente, False en caso contrario
    """
    try:
        # Verificar si ya existe una entrada para esta URL
        cursor.execute("""
            SELECT id FROM feeds 
            WHERE entity_type = ? AND entity_id = ? AND post_url = ?
        """, (entity_type, entity_id, post_url))
        
        existing = cursor.fetchone()
        
        # Si existe, actualizar
        if existing:
            cursor.execute("""
                UPDATE feeds 
                SET feed_name = ?, post_title = ?, post_date = ?, content = ?, added_date = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (feed_name, post_title, post_date, content, existing['id']))
            print(f"Actualizada rese√±a existente para {post_url}")
        else:
            # Si no existe, insertar nuevo
            cursor.execute("""
                INSERT INTO feeds (entity_type, entity_id, feed_name, post_title, post_url, post_date, content, added_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (entity_type, entity_id, feed_name, post_title, post_url, post_date, content))
            print(f"Insertada nueva rese√±a para {post_url}")
        
        conn.commit()
        return True
    except sqlite3.Error as e:
        print(f"Error al guardar en base de datos: {e}")
        conn.rollback()
        return False

def extraer_contenido_con_aclarar(url, service_type):
    """
    Extrae el contenido de una URL usando aclarar_contenido.py
    
    Args:
        url (str): URL de la p√°gina
        service_type (str): Tipo de servicio a usar ('five_filters', 'mercury', 'readability')
        
    Returns:
        str: Contenido extra√≠do o None si hubo un error
    """
    try:
        raw_content = tools.aclarar_contenido.get_full_content(url, service_type)
        
        # Procesar seg√∫n el tipo de servicio
        if service_type in ['mercury', 'readability']:
            # Estos servicios devuelven JSON
            if isinstance(raw_content, dict):
                if service_type == 'mercury' and 'content' in raw_content:
                    # Convertir HTML a texto plano b√°sico
                    soup = BeautifulSoup(raw_content['content'], 'html.parser')
                    return soup.get_text(separator='\n\n')
                elif service_type == 'readability' and 'content' in raw_content:
                    soup = BeautifulSoup(raw_content['content'], 'html.parser')
                    return soup.get_text(separator='\n\n')
            
            # Fall√≥ la conversi√≥n o estructura inesperada
            print(f"Estructura inesperada en la respuesta de {service_type}")
            return None
        else:
            # Five Filters devuelve texto
            return raw_content
    except Exception as e:
        print(f"Error al extraer contenido con aclarar_contenido.py: {e}")
        traceback.print_exc()
        return None





# METACRITIC

# Funci√≥n para crear la tabla album_metacritic
def crear_tabla_metacritic(cursor):
    """
    Crea la tabla album_metacritic si no existe
    """
    try:
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS album_metacritic (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                album_id INTEGER NOT NULL,
                metascore INTEGER,
                num_critics INTEGER DEFAULT 0,
                positive_reviews INTEGER DEFAULT 0,
                mixed_reviews INTEGER DEFAULT 0,
                negative_reviews INTEGER DEFAULT 0,
                metacritic_url TEXT,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (album_id) REFERENCES albums(id),
                UNIQUE(album_id)
            )
        """)
        print("Tabla album_metacritic creada o verificada")
        return True
    except sqlite3.Error as e:
        print(f"Error al crear tabla album_metacritic: {e}")
        return False




def guardar_datos_metacritic(conn, cursor, album_id, datos_metacritic):
    """
    Guarda los datos de Metacritic en la base de datos
    
    Args:
        conn: Conexi√≥n a la base de datos
        cursor: Cursor de la base de datos
        album_id (int): ID del √°lbum
        datos_metacritic (dict): Datos extra√≠dos de Metacritic
        
    Returns:
        bool: True si se guard√≥ correctamente
    """
    try:
        # Verificar si ya existe
        cursor.execute("SELECT id FROM album_metacritic WHERE album_id = ?", (album_id,))
        existing = cursor.fetchone()
        
        if existing:
            # Actualizar
            cursor.execute("""
                UPDATE album_metacritic 
                SET metascore = ?, num_critics = ?, positive_reviews = ?, 
                    mixed_reviews = ?, negative_reviews = ?, metacritic_url = ?,
                    last_updated = CURRENT_TIMESTAMP
                WHERE album_id = ?
            """, (
                datos_metacritic['metascore'],
                datos_metacritic['num_critics'],
                datos_metacritic['positive_reviews'],
                datos_metacritic['mixed_reviews'],
                datos_metacritic['negative_reviews'],
                datos_metacritic['metacritic_url'],
                album_id
            ))
            print(f"Datos de Metacritic actualizados para √°lbum ID {album_id}")
        else:
            # Insertar nuevo
            cursor.execute("""
                INSERT INTO album_metacritic 
                (album_id, metascore, num_critics, positive_reviews, mixed_reviews, negative_reviews, metacritic_url)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                album_id,
                datos_metacritic['metascore'],
                datos_metacritic['num_critics'],
                datos_metacritic['positive_reviews'],
                datos_metacritic['mixed_reviews'],
                datos_metacritic['negative_reviews'],
                datos_metacritic['metacritic_url']
            ))
            print(f"Datos de Metacritic insertados para √°lbum ID {album_id}")
        
        conn.commit()
        return True
        
    except sqlite3.Error as e:
        print(f"Error guardando datos de Metacritic: {e}")
        conn.rollback()
        return False



def limpiar_nombre_para_url(nombre):
    """
    Limpia un nombre de artista o √°lbum para usarlo en URLs de Metacritic
    
    Args:
        nombre (str): Nombre a limpiar
        
    Returns:
        str: Nombre limpio para URL
    """
    # Convertir a min√∫sculas
    nombre = nombre.lower()
    
    # Reemplazar caracteres especiales y espacios
    nombre = re.sub(r'[^\w\s-]', '', nombre)  # Eliminar caracteres especiales excepto guiones y espacios
    nombre = re.sub(r'\s+', '-', nombre)       # Reemplazar espacios con guiones
    nombre = re.sub(r'-+', '-', nombre)        # Eliminar guiones m√∫ltiples
    nombre = nombre.strip('-')                 # Eliminar guiones al inicio y final
    
    return nombre

def construir_url_metacritic_directa(artist_name, album_name):
    """
    Construye la URL directa de Metacritic basada en el patr√≥n conocido
    
    Args:
        artist_name (str): Nombre del artista
        album_name (str): Nombre del √°lbum
        
    Returns:
        str: URL directa de Metacritic
    """
    artist_clean = limpiar_nombre_para_url(artist_name)
    album_clean = limpiar_nombre_para_url(album_name)
    
    # Patr√≥n de Metacritic: /music/[album-name]/[artist-name]
    url = f"https://www.metacritic.com/music/{album_clean}/{artist_clean}"
    
    return url

def verificar_url_metacritic(url):
    """
    Versi√≥n mejorada de verificaci√≥n que es m√°s tolerante
    
    Args:
        url (str): URL a verificar
        
    Returns:
        bool: True si la URL existe y es v√°lida
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.head(url, headers=headers, timeout=15, allow_redirects=True)
        
        # Aceptar tanto 200 como algunos otros c√≥digos v√°lidos
        if response.status_code in [200, 301, 302]:
            # Verificar que la URL final siga siendo de Metacritic
            if 'metacritic.com' in response.url and '/music/' in response.url:
                return True
        
        return False
    except:
        return False

def buscar_album_metacritic_mejorado(artist_name, album_name):
    """
    Versi√≥n mejorada con m√∫ltiples m√©todos y anti-detecci√≥n
    """
    print(f"Buscando en Metacritic: {artist_name} - {album_name}")
    
    # M√©todo 1: URL directa (m√°s r√°pido, sin scraping)
    print("Probando URL directa primero...")
    url_directa = construir_url_metacritic_directa(artist_name, album_name)
    if verificar_url_metacritic(url_directa):
        print(f"‚úì Encontrado en URL directa: {url_directa}")
        return url_directa
    
    # Delay aleatorio antes de scraping
    delay = random.uniform(2, 5)
    print(f"Esperando {delay:.1f} segundos...")
    time.sleep(delay)
    
    # M√©todo 2: DuckDuckGo con anti-detecci√≥n
    print("Probando con DuckDuckGo...")
    metacritic_url = buscar_metacritic_con_duckduckgo(artist_name, album_name)
    if metacritic_url:
        return metacritic_url
    
    # M√©todo 3: Google como fallback
    print("Probando con Google como fallback...")
    metacritic_url = buscar_metacritic_con_google_scraping(artist_name, album_name)
    if metacritic_url:
        return metacritic_url
    
    # M√©todo 4: Variaciones de URL directa
    print("Probando variaciones de URL directa...")
    variaciones_album = generar_variaciones_nombre(album_name)[:2]
    variaciones_artista = generar_variaciones_nombre(artist_name)[:2]
    
    for album_var in variaciones_album:
        for artist_var in variaciones_artista:
            url_directa = construir_url_metacritic_directa(artist_var, album_var)
            
            # Delay peque√±o entre intentos
            time.sleep(random.uniform(0.5, 1.5))
            
            if verificar_url_metacritic(url_directa):
                print(f"‚úì Encontrado en variaci√≥n directa: {url_directa}")
                return url_directa
    
    print(f"‚úó No se encontr√≥ el √°lbum en Metacritic")
    return None
def procesar_albums(db_path, content_service, archivo_errores=None, inicio_id=0, lote=50, pausa=2):
    """
    Procesa m√∫ltiples √°lbumes para buscar sus rese√±as
    """
    conn, cursor = conectar_db(db_path)
    
    # Verificar y crear todas las tablas necesarias al inicio
    if not verificar_y_crear_tablas(conn, cursor):
        print("‚ùå Error al verificar/crear tablas. Abortando.")
        conn.close()
        return 0, 0
    
    # Obtener todos los √°lbumes
    todos_albums = obtener_albums(cursor)
    
    # Filtrar √°lbumes para comenzar desde inicio_id
    albums_a_procesar = [a for a in todos_albums if a['id'] >= inicio_id]
    
    print(f"Se procesar√°n {len(albums_a_procesar)} √°lbumes, comenzando desde ID {inicio_id}")
    
    # Estad√≠sticas
    albums_procesados = 0
    resenas_totales = 0
    
    # Procesar por lotes
    for i in range(0, len(albums_a_procesar), lote):
        lote_actual = albums_a_procesar[i:i+lote]
        print(f"\n=== Procesando lote {i//lote + 1} ({len(lote_actual)} √°lbumes) ===")
        
        for album in lote_actual:
            print(f"\nProcesando √°lbum ID {album['id']}: {album['artist']} - {album['name']}")
            
            # Buscar rese√±as para este √°lbum
            resenas = buscar_album_en_db(conn, cursor, album['id'], album['name'], 
                                       album['artist'], content_service, archivo_errores)
            
            # Actualizar estad√≠sticas
            albums_procesados += 1
            resenas_totales += resenas
            
            # Pausa para evitar sobrecargar el servidor
            if albums_procesados < len(albums_a_procesar):
                print(f"Esperando {pausa} segundos antes de la pr√≥xima b√∫squeda...")
                time.sleep(pausa)
        
        # Resumen del lote
        print(f"\n--- Resumen lote {i//lote + 1} ---")
        print(f"√Ålbumes procesados: {albums_procesados}")
        print(f"Rese√±as encontradas: {resenas_totales}")
    
    # Cerrar conexi√≥n
    conn.close()
    
    return albums_procesados, resenas_totales

# A√±adir estas opciones a tu configuraci√≥n
def procesar_albums_con_anti_deteccion(db_path, content_service, config):
    """
    Versi√≥n mejorada del procesador con anti-detecci√≥n
    """
    # Nuevas configuraciones
    use_proxy = config.get('use_proxy', False)
    proxy_list = config.get('proxy_list', [])
    max_intentos_por_sesion = config.get('max_intentos_por_sesion', 20)
    pausa_entre_sesiones = config.get('pausa_entre_sesiones', 300)  # 5 minutos
    
    intentos_sesion = 0
    intentos_fallidos = 0
    
    for album in albums_a_procesar:
        # Verificar l√≠mite de intentos por sesi√≥n
        if intentos_sesion >= max_intentos_por_sesion:
            print(f"\n‚è∏Ô∏è L√≠mite de sesi√≥n alcanzado. Pausando {pausa_entre_sesiones/60:.1f} minutos...")
            time.sleep(pausa_entre_sesiones)
            intentos_sesion = 0
            intentos_fallidos = 0
        
        # Procesar √°lbum con manejo de errores
        try:
            resultado = buscar_album_metacritic_mejorado(
                album['artist'], 
                album['name']
            )
            
            if resultado:
                intentos_fallidos = 0  # Reset en √©xito
            else:
                intentos_fallidos += 1
                manejar_rate_limiting(intentos_fallidos)
            
            intentos_sesion += 1
            
        except Exception as e:
            print(f"Error procesando √°lbum: {e}")
            intentos_fallidos += 1
            manejar_rate_limiting(intentos_fallidos)


def manejar_rate_limiting(intentos_fallidos):
    """
    Implementa backoff exponencial para rate limiting
    
    Args:
        intentos_fallidos (int): N√∫mero de intentos fallidos consecutivos
    """
    if intentos_fallidos > 0:
        # Backoff exponencial con jitter
        base_delay = min(2 ** intentos_fallidos, 60)  # M√°ximo 60 segundos
        jitter = random.uniform(0, base_delay * 0.3)
        total_delay = base_delay + jitter
        
        print(f"‚è≥ Rate limiting: esperando {total_delay:.1f} segundos (intento {intentos_fallidos})...")
        time.sleep(total_delay)


def buscar_album_metacritic_fallback(artist_name, album_name):
    """
    B√∫squeda de fallback usando el buscador de Metacritic
    
    Args:
        artist_name (str): Nombre del artista
        album_name (str): Nombre del √°lbum
        
    Returns:
        str or None: URL del √°lbum en Metacritic si se encuentra
    """
    query = f"{artist_name} {album_name}".strip()
    search_url = f"https://www.metacritic.com/search/music/{quote_plus(query)}/results"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print(f"Buscando con el buscador de Metacritic: {search_url}")
        response = requests.get(search_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Buscar enlaces de resultados de m√∫sica
        result_links = soup.find_all('a', href=True)
        
        for link in result_links:
            href = link['href']
            if '/music/' in href and href.count('/') >= 3:  # Formato /music/album/artist
                # Construir URL completa si es relativa
                if href.startswith('/'):
                    result_url = f"https://www.metacritic.com{href}"
                else:
                    result_url = href
                
                # Verificar que no sea una p√°gina de artista
                if not result_url.endswith('/critic-reviews/') and '/artist/' not in result_url:
                    print(f"Encontrado resultado de b√∫squeda: {result_url}")
                    
                    # Verificar que la URL sea v√°lida
                    if verificar_url_metacritic(result_url):
                        return result_url
        
        print(f"No se encontr√≥ '{album_name}' de '{artist_name}' en el buscador de Metacritic")
        return None
        
    except Exception as e:
        print(f"Error en b√∫squeda de fallback en Metacritic: {e}")
        return None

def extraer_datos_metacritic_mejorado(metacritic_url):
    """
    Versi√≥n mejorada para extraer datos de Metacritic con mejor detecci√≥n de elementos
    
    Args:
        metacritic_url (str): URL de la p√°gina del √°lbum en Metacritic
        
    Returns:
        dict: Diccionario con los datos extra√≠dos o None si hay error
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print(f"Extrayendo datos de: {metacritic_url}")
        response = requests.get(metacritic_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        datos = {
            'metascore': None,
            'num_critics': 0,
            'positive_reviews': 0,
            'mixed_reviews': 0,
            'negative_reviews': 0,
            'metacritic_url': metacritic_url
        }
        
        # Extraer Metascore - m√∫ltiples selectores posibles
        metascore_selectors = [
            'div.metascore_w',
            'span.metascore_w', 
            '.c-siteReviewScore_background-critic_medium',
            '.c-siteReviewScore_background-critic_large',
            '.metascore_w'
        ]
        
        metascore_elem = None
        for selector in metascore_selectors:
            metascore_elem = soup.select_one(selector)
            if metascore_elem:
                break
        
        if metascore_elem:
            try:
                metascore_text = metascore_elem.get_text().strip()
                datos['metascore'] = int(re.search(r'\d+', metascore_text).group())
                print(f"Metascore encontrado: {datos['metascore']}")
            except (ValueError, AttributeError):
                print("No se pudo extraer el Metascore del elemento encontrado")
        
        # Buscar informaci√≥n de cr√≠ticos y distribuci√≥n
        page_text = soup.get_text()
        
        # Buscar "Based on X Critic Reviews" o "X Critic Reviews"
        critics_patterns = [
            r'Based on (\d+) Critic Review',
            r'(\d+) Critic Review',
            r'(\d+) critic review'
        ]
        
        for pattern in critics_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                datos['num_critics'] = int(match.group(1))
                print(f"N√∫mero de cr√≠ticos: {datos['num_critics']}")
                break
        
        # Buscar distribuci√≥n de rese√±as
        # Patrones para buscar distribuci√≥n
        positive_patterns = [
            r'(\d+)\s+Positive',
            r'Positive[:\s]+(\d+)',
            r'(\d+)\s+positive'
        ]
        
        mixed_patterns = [
            r'(\d+)\s+Mixed',
            r'Mixed[:\s]+(\d+)',
            r'(\d+)\s+mixed'
        ]
        
        negative_patterns = [
            r'(\d+)\s+Negative',
            r'Negative[:\s]+(\d+)',
            r'(\d+)\s+negative'
        ]
        
        # Buscar distribuci√≥n con patrones
        for pattern in positive_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                datos['positive_reviews'] = int(match.group(1))
                break
        
        for pattern in mixed_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                datos['mixed_reviews'] = int(match.group(1))
                break
        
        for pattern in negative_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                datos['negative_reviews'] = int(match.group(1))
                break
        
        print(f"Datos extra√≠dos: Metascore={datos['metascore']}, Critics={datos['num_critics']}, Positive={datos['positive_reviews']}, Mixed={datos['mixed_reviews']}, Negative={datos['negative_reviews']}")
        
        return datos
        
    except Exception as e:
        print(f"Error extrayendo datos de Metacritic: {e}")
        return None

# Actualizar la funci√≥n principal para usar las versiones mejoradas
def buscar_album_en_db_con_metacritic_mejorado(conn, cursor, album_id, album_name, artist_name, content_service, include_metacritic=True, archivo_errores=None):
    """
    Versi√≥n mejorada con mejor b√∫squeda en Metacritic
    """
    enlaces_guardados = 0
    
    # B√∫squeda original en AnyDecentMusic
    enlaces_guardados += buscar_album_en_db(conn, cursor, album_id, album_name, artist_name, content_service, archivo_errores)
    
    # Nueva b√∫squeda en Metacritic
    if include_metacritic:
        print(f"\n--- Buscando en Metacritic: {artist_name} - {album_name} ---")
        
        # Usar la funci√≥n mejorada de b√∫squeda
        metacritic_url = buscar_album_metacritic_mejorado(artist_name, album_name)
        
        if metacritic_url:
            # Extraer datos de Metacritic con la funci√≥n mejorada
            datos_metacritic = extraer_datos_metacritic_mejorado(metacritic_url)
            
            if datos_metacritic:
                # Guardar en la tabla album_metacritic
                if guardar_datos_metacritic(conn, cursor, album_id, datos_metacritic):
                    print(f"‚úì Datos de Metacritic guardados para {artist_name} - {album_name}")
                
                # Tambi√©n guardar la p√°gina como una rese√±a en feeds si tiene contenido √∫til
                if datos_metacritic['metascore'] is not None:
                    post_title = f"Metacritic Reviews - {artist_name} - {album_name} (Score: {datos_metacritic['metascore']})"
                    content = f"Metacritic Score: {datos_metacritic['metascore']}/100\n"
                    content += f"Based on {datos_metacritic['num_critics']} critic reviews\n"
                    content += f"Positive: {datos_metacritic['positive_reviews']}, "
                    content += f"Mixed: {datos_metacritic['mixed_reviews']}, "
                    content += f"Negative: {datos_metacritic['negative_reviews']}"
                    
                    post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    if guardar_feed(conn, cursor, 'album', album_id, 'metacritic.com', 
                                  post_title, metacritic_url, post_date, content):
                        enlaces_guardados += 1
                        print(f"‚úì Rese√±a de Metacritic guardada en feeds")
            else:
                print("No se pudieron extraer datos de la p√°gina de Metacritic")
        else:
            print(f"‚úó No se encontr√≥ el √°lbum en Metacritic")
    
    return enlaces_guardados

# Actualizar la funci√≥n procesar_albums_con_metacritic para usar la versi√≥n mejorada
def procesar_albums_con_metacritic_mejorado(db_path, content_service, include_metacritic=True, archivo_errores=None, inicio_id=0, lote=50, pausa=2):
    """
    Versi√≥n mejorada de procesar_albums que incluye mejor b√∫squeda en Metacritic
    """
    conn, cursor = conectar_db(db_path)
    
    # Crear tabla de Metacritic si no existe
    if include_metacritic:
        crear_tabla_metacritic(cursor)
        conn.commit()
    
    # Obtener todos los √°lbumes
    todos_albums = obtener_albums(cursor)
    
    # Filtrar √°lbumes para comenzar desde inicio_id
    albums_a_procesar = [a for a in todos_albums if a['id'] >= inicio_id]
    
    print(f"Se procesar√°n {len(albums_a_procesar)} √°lbumes, comenzando desde ID {inicio_id}")
    if include_metacritic:
        print("‚úì Incluye b√∫squeda MEJORADA en Metacritic")
    
    # Estad√≠sticas
    albums_procesados = 0
    resenas_totales = 0
    metacritic_encontrados = 0
    
    # Procesar por lotes
    for i in range(0, len(albums_a_procesar), lote):
        lote_actual = albums_a_procesar[i:i+lote]
        print(f"\n{'='*60}")
        print(f"PROCESANDO LOTE {i//lote + 1} ({len(lote_actual)} √°lbumes)")
        print(f"{'='*60}")
        
        for album in lote_actual:
            print(f"\nüéµ Procesando √°lbum ID {album['id']}: {album['artist']} - {album['name']}")
            
            # Buscar rese√±as para este √°lbum (incluyendo Metacritic mejorado)
            resenas = buscar_album_en_db_con_metacritic_mejorado(
                conn, cursor, album['id'], album['name'], album['artist'], 
                content_service, include_metacritic, archivo_errores
            )
            
            # Verificar si se encontr√≥ en Metacritic
            if include_metacritic:
                cursor.execute("SELECT metascore FROM album_metacritic WHERE album_id = ?", (album['id'],))
                if cursor.fetchone():
                    metacritic_encontrados += 1
            
            # Actualizar estad√≠sticas
            albums_procesados += 1
            resenas_totales += resenas
            
            # Pausa para evitar sobrecargar el servidor
            if albums_procesados < len(albums_a_procesar):
                print(f"‚è≥ Esperando {pausa} segundos antes de la pr√≥xima b√∫squeda...")
                time.sleep(pausa)
        
        # Resumen del lote
        print(f"\n--- RESUMEN LOTE {i//lote + 1} ---")
        print(f"‚úì √Ålbumes procesados: {albums_procesados}")
        print(f"‚úì Rese√±as encontradas: {resenas_totales}")
        if include_metacritic:
            print(f"‚úì Encontrados en Metacritic: {metacritic_encontrados}")
        print(f"{'='*40}")
    
    # Cerrar conexi√≥n
    conn.close()
    
    return albums_procesados, resenas_totales, metacritic_encontrados

def extraer_enlaces_reviews_metacritic(metacritic_url):
    """
    Extrae todos los enlaces a rese√±as individuales de una p√°gina de Metacritic
    
    Args:
        metacritic_url (str): URL de la p√°gina del √°lbum en Metacritic
        
    Returns:
        tuple: (enlaces_validos, enlaces_error) donde ambos son listas de enlaces
    """
    print(f"Extrayendo enlaces de rese√±as de Metacritic: {metacritic_url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(metacritic_url, headers=headers, timeout=15)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Error al acceder a Metacritic: {e}")
        return [], []
    
    soup = BeautifulSoup(response.text, 'html.parser')
    
    enlaces_validos = []
    enlaces_error = []
    
    # Buscar rese√±as de cr√≠ticos - m√∫ltiples selectores posibles
    review_selectors = [
        # Selectores modernos de Metacritic
        '.c-siteReview',
        '.review.critic_review',
        '.critic_reviews .review',
        # Selectores cl√°sicos
        '.critic_review',
        '.review_wrap',
        '.review_section .review'
    ]
    
    reviews_found = []
    for selector in review_selectors:
        reviews_found = soup.select(selector)
        if reviews_found:
            print(f"Encontradas {len(reviews_found)} rese√±as usando selector: {selector}")
            break
    
    if not reviews_found:
        print("No se encontraron rese√±as en la p√°gina")
        return [], []
    
    for idx, review in enumerate(reviews_found, 1):
        try:
            # Buscar enlace externo a la rese√±a original
            link_selectors = [
                'a.c-siteReview_externalLink',
                'a[href*="http"]:not([href*="metacritic.com"])',  # Enlaces externos
                '.review_critic a[href*="http"]',
                '.source a',
                'a.external_link',
                'a[target="_blank"]'
            ]
            
            enlace_review = None
            for link_selector in link_selectors:
                enlace_review = review.select_one(link_selector)
                if enlace_review and enlace_review.has_attr('href'):
                    break
            
            if not enlace_review or not enlace_review.has_attr('href'):
                continue
                
            url_review = enlace_review['href']
            
            # Verificar que sea una URL externa v√°lida
            if not url_review.startswith('http'):
                continue
            if 'metacritic.com' in url_review:
                continue
                
            # Obtener informaci√≥n de la publicaci√≥n
            publication_selectors = [
                '.c-siteReview_source',
                '.review_critic .source',
                '.source',
                '.critic .source',
                '.review_publication'
            ]
            
            publication_elem = None
            for pub_selector in publication_selectors:
                publication_elem = review.select_one(pub_selector)
                if publication_elem:
                    break
            
            publication_name = publication_elem.text.strip() if publication_elem else extraer_dominio(url_review)
            
            # Obtener puntuaci√≥n si est√° disponible
            score_selectors = [
                '.c-siteReview_score',
                '.metascore_w',
                '.review_grade',
                '.score'
            ]
            
            score_elem = None
            for score_selector in score_selectors:
                score_elem = review.select_one(score_selector)
                if score_elem:
                    break
            
            score = score_elem.text.strip() if score_elem else None
            
            print(f"Encontrada rese√±a {idx}: {publication_name} - {url_review}")
            
            # Verificar si el enlace est√° activo
            success, resp_or_error, final_url = validar_y_hacer_peticion(url_review, headers)
            
            if success and resp_or_error.status_code == 200:
                estado = "Activo"
                if final_url != url_review:
                    estado = f"Redirigido a: {final_url}"
                    url_review = final_url
                
                enlaces_validos.append({
                    'numero': idx,
                    'publication': publication_name,
                    'url': url_review,
                    'estado': estado,
                    'score': score
                })
            elif success and resp_or_error.status_code in [404, 403]:
                # Enlaces muertos, no los registramos como error
                print(f"Enlace muerto (HTTP {resp_or_error.status_code}): {url_review}")
            else:
                error_msg = resp_or_error if not success else f"HTTP {resp_or_error.status_code}"
                enlaces_error.append({
                    'numero': idx,
                    'publication': publication_name,
                    'url': url_review,
                    'estado': error_msg,
                    'score': score
                })
                
        except Exception as e:
            print(f"Error procesando rese√±a {idx}: {e}")
            continue
    
    print(f"Total de enlaces v√°lidos encontrados: {len(enlaces_validos)}")
    print(f"Total de enlaces con errores: {len(enlaces_error)}")
    
    return enlaces_validos, enlaces_error




def procesar_reviews_metacritic_mejorado(conn, cursor, album_id, metacritic_url, content_service):
    """
    Versi√≥n mejorada que procesa rese√±as de Metacritic con filtrado de URLs
    """
    enlaces_validos, enlaces_error = extraer_enlaces_reviews_metacritic(metacritic_url)
    
    # Filtrar enlaces v√°lidos para rese√±as
    enlaces_validos = filtrar_enlaces_validos_para_reviews(enlaces_validos)
    
    if enlaces_error:
        print(f"Se encontraron {len(enlaces_error)} enlaces con errores en Metacritic")
    
    reviews_guardadas = 0
    
    for enlace in enlaces_validos:
        url = enlace['url']
        publication = enlace['publication']
        score = enlace.get('score')
        
        print(f"Procesando rese√±a de {publication}: {url}")
        
        try:
            # Usar la nueva funci√≥n de extracci√≥n espec√≠fica por sitio
            contenido = extraer_contenido_segun_sitio(url, content_service)
            
            if contenido:
                # Crear t√≠tulo para la rese√±a
                post_title = f"{publication} Review"
                if score:
                    post_title += f" (Score: {score})"
                
                # Fecha actual
                post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # Guardar en la tabla feeds
                feed_name = extraer_dominio(url)
                
                guardado = guardar_feed(
                    conn, cursor, 'album', album_id, feed_name,
                    post_title, url, post_date, contenido
                )
                
                if guardado:
                    reviews_guardadas += 1
                    print(f"‚úì Rese√±a guardada: {publication} - {post_title}")
                else:
                    print(f"‚úó Error guardando rese√±a de {publication}")
            else:
                print(f"‚úó No se pudo extraer contenido de {publication}: {url}")
                
        except Exception as e:
            print(f"‚úó Error procesando rese√±a de {publication}: {e}")
            continue
    
    print(f"Total de rese√±as guardadas desde Metacritic: {reviews_guardadas}")
    return reviews_guardadas




def buscar_album_en_db_con_metacritic_completo_mejorado(conn, cursor, album_id, album_name, artist_name, content_service, include_metacritic=True, archivo_errores=None):
    """
    Versi√≥n mejorada completa que extrae tanto datos de Metacritic como todas sus rese√±as individuales con filtrado
    """
    enlaces_guardados = 0
    
    # B√∫squeda mejorada en AnyDecentMusic
    try:
        enlaces_guardados += buscar_album_en_db_mejorado(conn, cursor, album_id, album_name, artist_name, content_service, archivo_errores)
    except Exception as e:
        print(f"Error en b√∫squeda AnyDecentMusic: {e}")
    
    # Nueva b√∫squeda completa en Metacritic
    if include_metacritic:
        print(f"\n{'='*60}")
        print(f"üéØ PROCESANDO METACRITIC: {artist_name} - {album_name}")
        print(f"{'='*60}")
        
        metacritic_url = buscar_album_metacritic_mejorado(artist_name, album_name)
        
        if metacritic_url:
            print(f"‚úì √Ålbum encontrado en Metacritic: {metacritic_url}")
            
            # 1. Extraer y guardar datos b√°sicos de Metacritic
            datos_metacritic = extraer_datos_metacritic_mejorado(metacritic_url)
            
            if datos_metacritic:
                if guardar_datos_metacritic(conn, cursor, album_id, datos_metacritic):
                    print(f"‚úì Datos b√°sicos de Metacritic guardados")
                
                # Guardar resumen como entrada en feeds
                if datos_metacritic['metascore'] is not None:
                    post_title = f"Metacritic Summary - {artist_name} - {album_name} (Score: {datos_metacritic['metascore']})"
                    content = f"Metacritic Score: {datos_metacritic['metascore']}/100\n"
                    content += f"Based on {datos_metacritic['num_critics']} critic reviews\n"
                    content += f"Distribution:\n"
                    content += f"  Positive (75-100): {datos_metacritic['positive_reviews']} reviews\n"
                    content += f"  Mixed (50-74): {datos_metacritic['mixed_reviews']} reviews\n"
                    content += f"  Negative (0-49): {datos_metacritic['negative_reviews']} reviews"
                    
                    post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    if guardar_feed(conn, cursor, 'album', album_id, 'metacritic.com', 
                                  post_title, metacritic_url, post_date, content):
                        enlaces_guardados += 1
            
            # 2. Procesar todas las rese√±as individuales de Metacritic con filtrado mejorado
            print(f"\nüîç Buscando rese√±as individuales en Metacritic...")
            reviews_procesadas = procesar_reviews_metacritic_mejorado(conn, cursor, album_id, metacritic_url, content_service)
            enlaces_guardados += reviews_procesadas
            
            if reviews_procesadas > 0:
                print(f"‚úì {reviews_procesadas} rese√±as individuales procesadas desde Metacritic")
            else:
                print(f"‚ö†Ô∏è  No se encontraron rese√±as individuales en Metacritic")
                
        else:
            print(f"‚úó No se encontr√≥ el √°lbum en Metacritic")
    
    return enlaces_guardados


def validar_y_hacer_peticion(url, headers, timeout=10):
    """
    Valida una URL y hace una petici√≥n HEAD de forma segura
    
    Args:
        url (str): URL a verificar
        headers (dict): Headers HTTP
        timeout (int): Timeout en segundos
        
    Returns:
        tuple: (success, response_or_error_message, final_url)
    """
    try:
        # Validar URL primero
        url_limpia = limpiar_url(url)
        if not url_limpia:
            return False, "URL inv√°lida", url
        
        # Hacer la petici√≥n
        resp = requests.head(
            url_limpia, 
            headers=headers, 
            allow_redirects=True, 
            timeout=timeout
        )
        
        return True, resp, resp.url
        
    except requests.exceptions.ConnectionError as e:
        return False, f"Error de conexi√≥n: {str(e)}", url
    except requests.exceptions.Timeout:
        return False, "Timeout de conexi√≥n", url
    except requests.exceptions.RequestException as e:
        return False, f"Error de petici√≥n: {str(e)}", url
    except Exception as e:
        return False, f"Error inesperado: {str(e)}", url

# AOTY

# FUNCIONES PARA ALBUM OF THE YEAR (AOTY)
# Agregar estas funciones a tu review_scrapper.py

def limpiar_nombre_para_aoty(nombre):
    """
    Limpia un nombre de artista o √°lbum para busqueda en AOTY
    
    Args:
        nombre (str): Nombre a limpiar
        
    Returns:
        str: Nombre limpio para b√∫squeda
    """
    # Convertir a min√∫sculas
    nombre = nombre.lower()
    
    # Reemplazar caracteres especiales
    nombre = re.sub(r'[^\w\s-]', ' ', nombre)  # Reemplazar especiales con espacios
    nombre = re.sub(r'\s+', ' ', nombre)       # Normalizar espacios
    nombre = nombre.strip()                    # Eliminar espacios al inicio/final
    
    return nombre


def verificar_url_aoty(url):
    """
    Verifica si una URL de AOTY existe y es v√°lida
    
    Args:
        url (str): URL a verificar
        
    Returns:
        bool: True si la URL existe y es v√°lida
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.head(url, headers=headers, timeout=10, allow_redirects=True)
        return response.status_code == 200
    except:
        return False

def extraer_datos_aoty(aoty_url):
    """
    Extrae los datos b√°sicos de puntuaci√≥n de una p√°gina de AOTY
    
    Args:
        aoty_url (str): URL de la p√°gina del √°lbum en AOTY
        
    Returns:
        dict: Diccionario con los datos extra√≠dos o None si hay error
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        print(f"Extrayendo datos de AOTY: {aoty_url}")
        response = requests.get(aoty_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        datos = {
            'user_score': None,
            'critic_score': None,
            'num_user_ratings': 0,
            'num_critic_ratings': 0,
            'aoty_url': aoty_url
        }
        
        # Extraer puntuaci√≥n de usuarios
        user_score_selectors = [
            '.userScore',
            '.albumUserScore',
            'div[class*="userScore"]',
            'span[class*="userScore"]'
        ]
        
        for selector in user_score_selectors:
            user_score_elem = soup.select_one(selector)
            if user_score_elem:
                try:
                    score_text = user_score_elem.get_text().strip()
                    # Extraer n√∫mero del texto (puede estar en formato "82" o "8.2/10")
                    score_match = re.search(r'(\d+(?:\.\d+)?)', score_text)
                    if score_match:
                        score = float(score_match.group(1))
                        # Convertir a escala 0-100 si es necesario
                        if score <= 10:
                            score = score * 10
                        datos['user_score'] = int(score)
                        print(f"User Score encontrado: {datos['user_score']}")
                        break
                except (ValueError, AttributeError):
                    continue
        
        # Extraer puntuaci√≥n de cr√≠ticos
        critic_score_selectors = [
            '.criticScore',
            '.albumCriticScore',
            'div[class*="criticScore"]',
            'span[class*="criticScore"]'
        ]
        
        for selector in critic_score_selectors:
            critic_score_elem = soup.select_one(selector)
            if critic_score_elem:
                try:
                    score_text = critic_score_elem.get_text().strip()
                    score_match = re.search(r'(\d+(?:\.\d+)?)', score_text)
                    if score_match:
                        score = float(score_match.group(1))
                        if score <= 10:
                            score = score * 10
                        datos['critic_score'] = int(score)
                        print(f"Critic Score encontrado: {datos['critic_score']}")
                        break
                except (ValueError, AttributeError):
                    continue
        
        # Extraer n√∫mero de ratings
        page_text = soup.get_text()
        
        # Buscar n√∫mero de user ratings
        user_ratings_patterns = [
            r'(\d+)\s+user\s+rating',
            r'(\d+)\s+rating',
            r'based\s+on\s+(\d+)\s+rating'
        ]
        
        for pattern in user_ratings_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                datos['num_user_ratings'] = int(match.group(1))
                print(f"N√∫mero de user ratings: {datos['num_user_ratings']}")
                break
        
        # Buscar n√∫mero de critic ratings
        critic_ratings_patterns = [
            r'(\d+)\s+critic\s+review',
            r'(\d+)\s+review'
        ]
        
        for pattern in critic_ratings_patterns:
            match = re.search(pattern, page_text, re.IGNORECASE)
            if match:
                datos['num_critic_ratings'] = int(match.group(1))
                print(f"N√∫mero de critic ratings: {datos['num_critic_ratings']}")
                break
        
        print(f"Datos extra√≠dos de AOTY: User={datos['user_score']}, Critic={datos['critic_score']}, User Ratings={datos['num_user_ratings']}, Critic Ratings={datos['num_critic_ratings']}")
        
        return datos
        
    except Exception as e:
        print(f"Error extrayendo datos de AOTY: {e}")
        return None

def extraer_enlaces_reviews_aoty(aoty_url):
    """
    Extrae todos los enlaces a rese√±as externas de una p√°gina de AOTY
    
    Args:
        aoty_url (str): URL de la p√°gina del √°lbum en AOTY
        
    Returns:
        tuple: (enlaces_validos, enlaces_error) donde ambos son listas de enlaces
    """
    print(f"Extrayendo enlaces de rese√±as de AOTY: {aoty_url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        response = requests.get(aoty_url, headers=headers, timeout=15)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Error al acceder a AOTY: {e}")
        return [], []
    
    soup = BeautifulSoup(response.text, 'html.parser')
    
    enlaces_validos = []
    enlaces_error = []
    
    # Buscar secci√≥n de cr√≠ticas/rese√±as
    review_selectors = [
        '.criticReview',
        '.reviewEntry',
        '.criticEntry',
        'div[class*="review"]',
        'div[class*="critic"]'
    ]
    
    reviews_found = []
    for selector in review_selectors:
        reviews_found = soup.select(selector)
        if reviews_found:
            print(f"Encontradas {len(reviews_found)} rese√±as usando selector: {selector}")
            break
    
    # Si no encontramos con selectores espec√≠ficos, buscar enlaces externos
    if not reviews_found:
        # Buscar enlaces que parezcan ser de rese√±as externas
        external_links = soup.find_all('a', href=True)
        potential_reviews = []
        
        for link in external_links:
            href = link['href']
            # Filtrar enlaces que parezcan rese√±as externas
            if (href.startswith('http') and 
                not any(domain in href for domain in ['albumoftheyear.org', 'facebook.com', 'twitter.com', 'instagram.com']) and
                any(keyword in href.lower() for keyword in ['review', 'music', 'album', 'pitchfork', 'rolling', 'guardian', 'nme'])):
                potential_reviews.append(link)
        
        reviews_found = potential_reviews[:10]  # Limitar a 10 resultados
        print(f"Encontrados {len(reviews_found)} enlaces externos potenciales")
    
    if not reviews_found:
        print("No se encontraron rese√±as en la p√°gina de AOTY")
        return [], []
    
    for idx, review in enumerate(reviews_found, 1):
        try:
            # Si es un enlace directo
            if review.name == 'a' and review.has_attr('href'):
                url_review = review['href']
                publication_name = review.get_text().strip() or extraer_dominio(url_review)
            else:
                # Buscar enlace dentro del elemento de rese√±a
                link_elem = review.find('a', href=True)
                if not link_elem:
                    continue
                    
                url_review = link_elem['href']
                
                # Buscar nombre de la publicaci√≥n
                publication_elem = review.find(class_=re.compile(r'publication|source|site'))
                if not publication_elem:
                    publication_elem = review.find('span')
                
                publication_name = publication_elem.get_text().strip() if publication_elem else extraer_dominio(url_review)
            
            # Verificar que sea una URL externa v√°lida
            if not url_review.startswith('http'):
                if url_review.startswith('//'):
                    url_review = 'https:' + url_review
                elif url_review.startswith('/'):
                    continue  # Enlace interno, saltarlo
                else:
                    url_review = 'https://' + url_review
            
            # Excluir enlaces internos de AOTY
            if 'albumoftheyear.org' in url_review:
                continue
            
            print(f"Encontrada rese√±a {idx}: {publication_name} - {url_review}")
            
            # Verificar si el enlace est√° activo
            success, resp_or_error, final_url = validar_y_hacer_peticion(url_review, headers)
            
            if success and resp_or_error.status_code == 200:
                estado = "Activo"
                if final_url != url_review:
                    estado = f"Redirigido a: {final_url}"
                    url_review = final_url
                
                enlaces_validos.append({
                    'numero': idx,
                    'publication': publication_name,
                    'url': url_review,
                    'estado': estado
                })
            elif success and resp_or_error.status_code in [404, 403]:
                # Enlaces muertos, no los registramos como error
                print(f"Enlace muerto (HTTP {resp_or_error.status_code}): {url_review}")
            else:
                error_msg = resp_or_error if not success else f"HTTP {resp_or_error.status_code}"
                enlaces_error.append({
                    'numero': idx,
                    'publication': publication_name,
                    'url': url_review,
                    'estado': error_msg
                })
                
        except Exception as e:
            print(f"Error procesando rese√±a {idx}: {e}")
            continue
    
    print(f"Total de enlaces v√°lidos encontrados en AOTY: {len(enlaces_validos)}")
    print(f"Total de enlaces con errores en AOTY: {len(enlaces_error)}")
    
    return enlaces_validos, enlaces_error

def crear_tabla_aoty(cursor):
    """
    Crea la tabla album_aoty si no existe
    """
    try:
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS album_aoty (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                album_id INTEGER NOT NULL,
                user_score INTEGER,
                critic_score INTEGER,
                num_user_ratings INTEGER DEFAULT 0,
                num_critic_ratings INTEGER DEFAULT 0,
                aoty_url TEXT,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (album_id) REFERENCES albums(id),
                UNIQUE(album_id)
            )
        """)
        print("Tabla album_aoty creada o verificada")
        return True
    except sqlite3.Error as e:
        print(f"Error al crear tabla album_aoty: {e}")
        return False

def guardar_datos_aoty(conn, cursor, album_id, datos_aoty):
    """
    Guarda los datos de AOTY en la base de datos
    
    Args:
        conn: Conexi√≥n a la base de datos
        cursor: Cursor de la base de datos
        album_id (int): ID del √°lbum
        datos_aoty (dict): Datos extra√≠dos de AOTY
        
    Returns:
        bool: True si se guard√≥ correctamente
    """
    try:
        # Verificar si ya existe
        cursor.execute("SELECT id FROM album_aoty WHERE album_id = ?", (album_id,))
        existing = cursor.fetchone()
        
        if existing:
            # Actualizar
            cursor.execute("""
                UPDATE album_aoty 
                SET user_score = ?, critic_score = ?, num_user_ratings = ?, 
                    num_critic_ratings = ?, aoty_url = ?,
                    last_updated = CURRENT_TIMESTAMP
                WHERE album_id = ?
            """, (
                datos_aoty['user_score'],
                datos_aoty['critic_score'],
                datos_aoty['num_user_ratings'],
                datos_aoty['num_critic_ratings'],
                datos_aoty['aoty_url'],
                album_id
            ))
            print(f"Datos de AOTY actualizados para √°lbum ID {album_id}")
        else:
            # Insertar nuevo
            cursor.execute("""
                INSERT INTO album_aoty 
                (album_id, user_score, critic_score, num_user_ratings, num_critic_ratings, aoty_url)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                album_id,
                datos_aoty['user_score'],
                datos_aoty['critic_score'],
                datos_aoty['num_user_ratings'],
                datos_aoty['num_critic_ratings'],
                datos_aoty['aoty_url']
            ))
            print(f"Datos de AOTY insertados para √°lbum ID {album_id}")
        
        conn.commit()
        return True
        
    except sqlite3.Error as e:
        print(f"Error guardando datos de AOTY: {e}")
        conn.rollback()
        return False



def procesar_reviews_aoty_mejorado(conn, cursor, album_id, aoty_url, content_service):
    """
    Versi√≥n mejorada que procesa rese√±as de AOTY con filtrado de URLs
    """
    enlaces_validos, enlaces_error = extraer_enlaces_reviews_aoty(aoty_url)
    
    # Filtrar enlaces v√°lidos para rese√±as
    enlaces_validos = filtrar_enlaces_validos_para_reviews(enlaces_validos)
    
    if enlaces_error:
        print(f"Se encontraron {len(enlaces_error)} enlaces con errores en AOTY")
    
    reviews_guardadas = 0
    
    for enlace in enlaces_validos:
        url = enlace['url']
        publication = enlace['publication']
        
        print(f"Procesando rese√±a de {publication}: {url}")
        
        try:
            # Usar la nueva funci√≥n de extracci√≥n espec√≠fica por sitio
            contenido = extraer_contenido_segun_sitio(url, content_service)
            
            if contenido:
                # Crear t√≠tulo para la rese√±a
                post_title = f"{publication} Review (via AOTY)"
                
                # Fecha actual
                post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # Guardar en la tabla feeds
                feed_name = extraer_dominio(url)
                
                guardado = guardar_feed(
                    conn, cursor, 'album', album_id, feed_name,
                    post_title, url, post_date, contenido
                )
                
                if guardado:
                    reviews_guardadas += 1
                    print(f"‚úì Rese√±a guardada: {publication} - {post_title}")
                else:
                    print(f"‚úó Error guardando rese√±a de {publication}")
            else:
                print(f"‚úó No se pudo extraer contenido de {publication}: {url}")
                
        except Exception as e:
            print(f"‚úó Error procesando rese√±a de {publication}: {e}")
            continue
    
    print(f"Total de rese√±as guardadas desde AOTY: {reviews_guardadas}")
    return reviews_guardadas




def buscar_album_en_db_completo_mejorado(conn, cursor, album_id, album_name, artist_name, content_service, 
                               include_metacritic=True, include_aoty=True, archivo_errores=None,
                               google_api_key=None, google_cx=None):
    """
    Versi√≥n mejorada completa con filtrado de URLs de streaming
    """
    enlaces_guardados = 0
    
    # 1. B√∫squeda mejorada en AnyDecentMusic
    try:
        print(f"\nüåç Buscando en AnyDecentMusic...")
        enlaces_guardados += buscar_album_en_db_mejorado(conn, cursor, album_id, album_name, artist_name, content_service, archivo_errores)
    except Exception as e:
        print(f"Error en b√∫squeda AnyDecentMusic: {e}")
    
    # 2. B√∫squeda completa en Metacritic
    if include_metacritic:
        print(f"\nüéØ Buscando en Metacritic...")
        try:
            metacritic_url = buscar_album_metacritic_mejorado(artist_name, album_name)
            
            if metacritic_url:
                print(f"‚úì √Ålbum encontrado en Metacritic: {metacritic_url}")
                
                # Extraer y guardar datos b√°sicos de Metacritic
                datos_metacritic = extraer_datos_metacritic_mejorado(metacritic_url)
                
                if datos_metacritic:
                    if guardar_datos_metacritic(conn, cursor, album_id, datos_metacritic):
                        print(f"‚úì Datos b√°sicos de Metacritic guardados")
                    
                    # Guardar resumen como entrada en feeds
                    if datos_metacritic['metascore'] is not None:
                        post_title = f"Metacritic Summary - {artist_name} - {album_name} (Score: {datos_metacritic['metascore']})"
                        content = f"Metacritic Score: {datos_metacritic['metascore']}/100\n"
                        content += f"Based on {datos_metacritic['num_critics']} critic reviews\n"
                        content += f"Distribution: +{datos_metacritic['positive_reviews']} ¬±{datos_metacritic['mixed_reviews']} -{datos_metacritic['negative_reviews']}"
                        
                        post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        
                        if guardar_feed(conn, cursor, 'album', album_id, 'metacritic.com', 
                                      post_title, metacritic_url, post_date, content):
                            enlaces_guardados += 1
                
                # Procesar rese√±as individuales de Metacritic con filtrado mejorado
                print(f"üîç Buscando rese√±as individuales en Metacritic...")
                reviews_procesadas = procesar_reviews_metacritic_mejorado(conn, cursor, album_id, metacritic_url, content_service)
                enlaces_guardados += reviews_procesadas
                
                if reviews_procesadas > 0:
                    print(f"‚úì {reviews_procesadas} rese√±as individuales procesadas desde Metacritic")
                else:
                    print(f"‚ö†Ô∏è  No se encontraron rese√±as individuales en Metacritic")
            else:
                print(f"‚úó No se encontr√≥ el √°lbum en Metacritic")
        except Exception as e:
            print(f"Error en b√∫squeda Metacritic: {e}")
    
    # 3. B√∫squeda completa en Album of the Year (AOTY)
    if include_aoty:
        print(f"\nüèÜ Buscando en Album of the Year...")
        try:
            aoty_url = buscar_album_aoty(artist_name, album_name, google_api_key, google_cx)
            
            if aoty_url:
                print(f"‚úì √Ålbum encontrado en AOTY: {aoty_url}")
                
                # Extraer y guardar datos b√°sicos de AOTY
                datos_aoty = extraer_datos_aoty(aoty_url)
                
                if datos_aoty:
                    if guardar_datos_aoty(conn, cursor, album_id, datos_aoty):
                        print(f"‚úì Datos b√°sicos de AOTY guardados")
                        print(f"   User Score: {datos_aoty['user_score']}")
                        print(f"   Critic Score: {datos_aoty['critic_score']}")
                    
                    # Guardar resumen como entrada en feeds
                    if datos_aoty['user_score'] is not None or datos_aoty['critic_score'] is not None:
                        scores_text = []
                        if datos_aoty['user_score']:
                            scores_text.append(f"User: {datos_aoty['user_score']}")
                        if datos_aoty['critic_score']:
                            scores_text.append(f"Critic: {datos_aoty['critic_score']}")
                        
                        post_title = f"AOTY Summary - {artist_name} - {album_name} ({'/'.join(scores_text)})"
                        content = f"Album of the Year Scores:\n"
                        if datos_aoty['user_score']:
                            content += f"User Score: {datos_aoty['user_score']}/100 (based on {datos_aoty['num_user_ratings']} ratings)\n"
                        if datos_aoty['critic_score']:
                            content += f"Critic Score: {datos_aoty['critic_score']}/100 (based on {datos_aoty['num_critic_ratings']} reviews)\n"
                        
                        post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        
                        if guardar_feed(conn, cursor, 'album', album_id, 'albumoftheyear.org', 
                                      post_title, aoty_url, post_date, content):
                            enlaces_guardados += 1
                
                # Procesar rese√±as individuales de AOTY con filtrado mejorado
                print(f"üîç Buscando rese√±as individuales en AOTY...")
                reviews_procesadas = procesar_reviews_aoty_mejorado(conn, cursor, album_id, aoty_url, content_service)
                enlaces_guardados += reviews_procesadas
                
                if reviews_procesadas > 0:
                    print(f"‚úì {reviews_procesadas} rese√±as individuales procesadas desde AOTY")
                else:
                    print(f"‚ö†Ô∏è  No se encontraron rese√±as individuales en AOTY")
            else:
                print(f"‚úó No se encontr√≥ el √°lbum en Album of the Year")
        except Exception as e:
            print(f"Error en b√∫squeda AOTY: {e}")
    
    return enlaces_guardados




def buscar_album_aoty_con_google(artist_name, album_name, google_api_key=None, google_cx=None):
    """
    Busca un √°lbum en Album of the Year usando Google Search como m√©todo principal
    
    Args:
        artist_name (str): Nombre del artista
        album_name (str): Nombre del √°lbum
        google_api_key (str): API key de Google (opcional)
        google_cx (str): Custom Search Engine ID (opcional)
        
    Returns:
        str or None: URL del √°lbum en AOTY si se encuentra
    """
    print(f"Buscando en Album of the Year: {artist_name} - {album_name}")
    
    # M√©todo 1: B√∫squeda con Google Custom Search (si est√° disponible)
    if google_api_key and google_cx:
        try:
            aoty_url = buscar_aoty_con_google_api(artist_name, album_name, google_api_key, google_cx)
            if aoty_url:
                return aoty_url
        except Exception as e:
            print(f"Error con Google API, probando m√©todo directo: {e}")
    
    # M√©todo 2: B√∫squeda directa simulando Google
    try:
        aoty_url = buscar_aoty_simulando_google(artist_name, album_name)
        if aoty_url:
            return aoty_url
    except Exception as e:
        print(f"Error con b√∫squeda simulada de Google: {e}")
    
    # M√©todo 3: B√∫squeda original en AOTY (fallback)
    try:
        return buscar_album_aoty_original(artist_name, album_name)
    except Exception as e:
        print(f"Error con b√∫squeda original AOTY: {e}")
        return None

def buscar_aoty_con_google_api(artist_name, album_name, api_key, cx):
    """
    Busca usando la API oficial de Google Custom Search
    """
    import requests
    
    query = f'site:albumoftheyear.org "{artist_name}" "{album_name}"'
    url = "https://www.googleapis.com/customsearch/v1"
    
    params = {
        'key': api_key,
        'cx': cx,
        'q': query,
        'num': 5
    }
    
    print(f"Buscando con Google API: {query}")
    
    response = requests.get(url, params=params, timeout=10)
    response.raise_for_status()
    
    data = response.json()
    
    if 'items' in data:
        for item in data['items']:
            url = item['link']
            if '/album/' in url and 'albumoftheyear.org' in url:
                if verificar_url_aoty(url):
                    print(f"‚úì Encontrado con Google API: {url}")
                    return url
    
    return None

def buscar_aoty_simulando_google(artist_name, album_name):
    """
    Busca simulando una b√∫squeda de Google con site:albumoftheyear.org
    """
    import requests
    from urllib.parse import quote_plus
    
    # Diferentes variaciones de la consulta
    queries = [
        f'site:albumoftheyear.org "{artist_name}" "{album_name}"',
        f'site:albumoftheyear.org {artist_name} {album_name}',
        f'site:albumoftheyear.org "{album_name}" {artist_name}',
        f'site:albumoftheyear.org {album_name}'
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    for query in queries:
        try:
            print(f"Probando consulta Google: {query}")
            
            # Usar DuckDuckGo como alternativa m√°s confiable
            search_url = f"https://duckduckgo.com/html/?q={quote_plus(query)}"
            
            response = requests.get(search_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Buscar enlaces en los resultados
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href']
                
                # Limpiar URL de DuckDuckGo (suelen estar en formato /l/?uddg=...)
                if '/l/?uddg=' in href:
                    import urllib.parse
                    try:
                        href = urllib.parse.unquote(href.split('uddg=')[1].split('&')[0])
                    except:
                        continue
                
                if 'albumoftheyear.org' in href and '/album/' in href:
                    # Limpiar URL
                    if href.startswith('http'):
                        clean_url = href.split('&')[0]  # Eliminar par√°metros adicionales
                        
                        if verificar_url_aoty(clean_url):
                            print(f"‚úì Encontrado con DuckDuckGo: {clean_url}")
                            return clean_url
            
            # Peque√±a pausa entre consultas
            import time
            time.sleep(1)
            
        except Exception as e:
            print(f"Error en consulta '{query}': {e}")
            continue
    
    return None

def buscar_album_aoty_original(artist_name, album_name):
    """
    Versi√≥n original de b√∫squeda directa en AOTY (como fallback)
    """
    print(f"Usando b√∫squeda directa en AOTY como √∫ltimo recurso...")
    
    # Limpiar nombres para la b√∫squeda
    artist_clean = limpiar_nombre_para_aoty(artist_name)
    album_clean = limpiar_nombre_para_aoty(album_name)
    
    # Crear query de b√∫squeda
    query = f"{artist_clean} {album_clean}".strip()
    search_url = f"https://www.albumoftheyear.org/search/albums/?q={quote_plus(query)}"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        print(f"URL de b√∫squeda directa: {search_url}")
        response = requests.get(search_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Intentar m√∫ltiples selectores para encontrar resultados
        selectors_to_try = [
            'div.albumBlock a[href*="/album/"]',
            'a[href*="/album/"]',
            '.searchResult a',
            '.albumEntry a',
            'div.album a',
            '.result a'
        ]
        
        album_links = []
        for selector in selectors_to_try:
            links = soup.select(selector)
            if links:
                album_links = links
                print(f"Encontrados {len(links)} enlaces con selector: {selector}")
                break
        
        # Si a√∫n no encontramos nada, buscar todos los enlaces que contengan /album/
        if not album_links:
            all_links = soup.find_all('a', href=True)
            album_links = [link for link in all_links if '/album/' in link.get('href', '')]
            print(f"Encontrados {len(album_links)} enlaces de √°lbum mediante b√∫squeda general")
        
        # Verificar cada resultado
        for link in album_links[:5]:  # Solo verificar los primeros 5
            href = link['href']
            
            if href.startswith('/'):
                result_url = f"https://www.albumoftheyear.org{href}"
            else:
                result_url = href
            
            if verificar_url_aoty(result_url):
                print(f"‚úì Primer resultado v√°lido en b√∫squeda directa: {result_url}")
                return result_url
        
        return None
        
    except Exception as e:
        print(f"Error en b√∫squeda directa AOTY: {e}")
        return None

# Actualizar la funci√≥n principal para usar la nueva b√∫squeda
def buscar_album_aoty(artist_name, album_name, google_api_key=None, google_cx=None):
    """
    Funci√≥n principal actualizada que usa los m√©todos mejorados
    """
    return buscar_album_aoty_mejorado_v2(artist_name, album_name, google_api_key, google_cx)




def es_url_valida_para_reviews(url):
    """
    Verifica si una URL es v√°lida para extraer rese√±as (evita URLs de streaming)
    
    Args:
        url (str): URL a verificar
        
    Returns:
        bool: True si es v√°lida para rese√±as, False si debe ser evitada
    """
    urls_a_evitar = [
        'music.apple.com',
        'open.spotify.com',
        'spotify.com/album',
        'spotify.com/artist',
        'music.amazon.com',
        'tidal.com',
        'deezer.com',
        'youtube.com/playlist',
        'youtube.com/channel',
        'bandcamp.com/track',  # Solo evitar tracks individuales, no √°lbumes
        'soundcloud.com/sets'  # Solo evitar sets, no tracks individuales
    ]
    
    url_lower = url.lower()
    
    for url_evitar in urls_a_evitar:
        if url_evitar in url_lower:
            print(f"‚ö†Ô∏è  URL evitada (streaming/no-review): {url}")
            return False
    
    return True

def extraer_contenido_segun_sitio(url, content_service):
    """
    Extrae contenido espec√≠fico seg√∫n el sitio web
    
    Args:
        url (str): URL de la p√°gina
        content_service (str): Servicio base para extraer contenido
        
    Returns:
        str: Contenido extra√≠do o None si hubo un error
    """
    try:
        # Verificar si es una URL que debemos evitar
        if not es_url_valida_para_reviews(url):
            return None
            
        domain = extraer_dominio(url)
        print(f"Extrayendo contenido de {domain}: {url}")
        
        # Extracciones espec√≠ficas por sitio
        if 'allmusic.com' in domain:
            return extraer_contenido_allmusic(url)
        elif 'sputnikmusic.com' in domain:
            return extraer_contenido_sputnikmusic(url)
        elif 'pitchfork.com' in domain:
            return extraer_contenido_pitchfork(url)
        elif 'rollingstone.com' in domain:
            return extraer_contenido_rollingstone(url)
        elif 'nme.com' in domain:
            return extraer_contenido_nme(url)
        else:
            # Para otros sitios, usar el m√©todo general
            return extraer_contenido_con_aclarar(url, content_service)
            
    except Exception as e:
        print(f"Error extrayendo contenido espec√≠fico de {url}: {e}")
        # Fallback al m√©todo general
        return extraer_contenido_con_aclarar(url, content_service)

def extraer_contenido_allmusic(url):
    """
    Extrae espec√≠ficamente las rese√±as de AllMusic
    
    Args:
        url (str): URL de AllMusic
        
    Returns:
        str: Contenido de la rese√±a o None
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Selectores espec√≠ficos para rese√±as de AllMusic
        review_selectors = [
            'div.review',
            'section.review',
            'div.editorial-review',
            'div[class*="review"]',
            'div.content .text',
            'section[class*="editorial"]',
            '.review-content',
            'div.album-review'
        ]
        
        review_content = None
        
        for selector in review_selectors:
            review_element = soup.select_one(selector)
            if review_element:
                # Limpiar el contenido
                # Remover elementos no deseados
                for unwanted in review_element.find_all(['script', 'style', 'nav', 'aside']):
                    unwanted.decompose()
                
                review_text = review_element.get_text(separator='\n\n').strip()
                
                # Verificar que tenga contenido sustancial (m√°s de 100 caracteres)
                if len(review_text) > 100:
                    review_content = review_text
                    print(f"‚úì Rese√±a extra√≠da de AllMusic usando selector: {selector}")
                    break
        
        # Si no encontramos rese√±a espec√≠fica, buscar p√°rrafos largos
        if not review_content:
            paragraphs = soup.find_all('p')
            long_paragraphs = []
            
            for p in paragraphs:
                text = p.get_text().strip()
                if len(text) > 150:  # P√°rrafos largos que probablemente sean rese√±a
                    long_paragraphs.append(text)
            
            if long_paragraphs:
                # Tomar los p√°rrafos m√°s largos (probablemente la rese√±a)
                review_content = '\n\n'.join(long_paragraphs[:3])
                print(f"‚úì Rese√±a extra√≠da de AllMusic usando p√°rrafos largos")
        
        if review_content and len(review_content) > 50:
            return f"AllMusic Review:\n\n{review_content}"
        else:
            print("‚ö†Ô∏è  No se encontr√≥ contenido de rese√±a espec√≠fico en AllMusic")
            return None
            
    except Exception as e:
        print(f"Error extrayendo contenido de AllMusic: {e}")
        return None

def extraer_contenido_sputnikmusic(url):
    """
    Extrae espec√≠ficamente las rese√±as de SputnikMusic
    
    Args:
        url (str): URL de SputnikMusic
        
    Returns:
        str: Contenido de la rese√±a o None
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Selectores espec√≠ficos para rese√±as de SputnikMusic
        review_selectors = [
            'div#review_text',
            'div.review_text',
            'div.reviewcontent',
            'div[class*="review"]',
            '#reviewbody',
            '.review-body',
            'div.main_review'
        ]
        
        review_content = None
        
        for selector in review_selectors:
            review_element = soup.select_one(selector)
            if review_element:
                # Limpiar el contenido
                for unwanted in review_element.find_all(['script', 'style', 'nav', 'aside', 'div.comments']):
                    unwanted.decompose()
                
                review_text = review_element.get_text(separator='\n\n').strip()
                
                # Verificar que tenga contenido sustancial
                if len(review_text) > 200:
                    review_content = review_text
                    print(f"‚úì Rese√±a extra√≠da de SputnikMusic usando selector: {selector}")
                    break
        
        # M√©todo alternativo: buscar el texto principal de la rese√±a
        if not review_content:
            # SputnikMusic suele tener la rese√±a en divs espec√≠ficos
            main_content = soup.find('div', {'id': 'main'}) or soup.find('div', {'class': 'main'})
            
            if main_content:
                # Buscar p√°rrafos largos dentro del contenido principal
                paragraphs = main_content.find_all('p')
                review_paragraphs = []
                
                for p in paragraphs:
                    text = p.get_text().strip()
                    if len(text) > 100:
                        review_paragraphs.append(text)
                
                if review_paragraphs:
                    review_content = '\n\n'.join(review_paragraphs)
                    print(f"‚úì Rese√±a extra√≠da de SputnikMusic usando p√°rrafos del contenido principal")
        
        if review_content and len(review_content) > 100:
            return f"SputnikMusic Review:\n\n{review_content}"
        else:
            print("‚ö†Ô∏è  No se encontr√≥ contenido de rese√±a espec√≠fico en SputnikMusic")
            return None
            
    except Exception as e:
        print(f"Error extrayendo contenido de SputnikMusic: {e}")
        return None

def extraer_contenido_pitchfork(url):
    """
    Extrae espec√≠ficamente las rese√±as de Pitchfork
    
    Args:
        url (str): URL de Pitchfork
        
    Returns:
        str: Contenido de la rese√±a o None
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Selectores espec√≠ficos para Pitchfork
        review_selectors = [
            'div.contents',
            'div.review-content',
            'div[class*="ReviewContent"]',
            'div[class*="review-text"]',
            'article div p',
            '.body-text',
            '.review-detail__text'
        ]
        
        review_content = None
        
        for selector in review_selectors:
            elements = soup.select(selector)
            if elements:
                # Para Pitchfork, concatenar varios p√°rrafos
                review_paragraphs = []
                for elem in elements:
                    text = elem.get_text().strip()
                    if len(text) > 50:
                        review_paragraphs.append(text)
                
                if review_paragraphs:
                    review_content = '\n\n'.join(review_paragraphs[:10])  # Limitar a 10 p√°rrafos
                    print(f"‚úì Rese√±a extra√≠da de Pitchfork usando selector: {selector}")
                    break
        
        if review_content and len(review_content) > 200:
            return f"Pitchfork Review:\n\n{review_content}"
        else:
            print("‚ö†Ô∏è  No se encontr√≥ contenido de rese√±a espec√≠fico en Pitchfork")
            return None
            
    except Exception as e:
        print(f"Error extrayendo contenido de Pitchfork: {e}")
        return None

def extraer_contenido_rollingstone(url):
    """
    Extrae espec√≠ficamente las rese√±as de Rolling Stone
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Selectores para Rolling Stone
        review_selectors = [
            'div.entry-content',
            'div.article-content',
            'div[class*="content"]',
            'article div p'
        ]
        
        for selector in review_selectors:
            elements = soup.select(selector)
            if elements:
                review_paragraphs = []
                for elem in elements:
                    text = elem.get_text().strip()
                    if len(text) > 50:
                        review_paragraphs.append(text)
                
                if review_paragraphs:
                    review_content = '\n\n'.join(review_paragraphs[:8])
                    print(f"‚úì Rese√±a extra√≠da de Rolling Stone")
                    return f"Rolling Stone Review:\n\n{review_content}"
        
        return None
        
    except Exception as e:
        print(f"Error extrayendo contenido de Rolling Stone: {e}")
        return None

def extraer_contenido_nme(url):
    """
    Extrae espec√≠ficamente las rese√±as de NME
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Selectores para NME
        review_selectors = [
            'div.ArticleBody',
            'div.entry-content',
            'div[class*="article-body"]',
            'article div p'
        ]
        
        for selector in review_selectors:
            elements = soup.select(selector)
            if elements:
                review_paragraphs = []
                for elem in elements:
                    text = elem.get_text().strip()
                    if len(text) > 50:
                        review_paragraphs.append(text)
                
                if review_paragraphs:
                    review_content = '\n\n'.join(review_paragraphs[:8])
                    print(f"‚úì Rese√±a extra√≠da de NME")
                    return f"NME Review:\n\n{review_content}"
        
        return None
        
    except Exception as e:
        print(f"Error extrayendo contenido de NME: {e}")
        return None





def filtrar_enlaces_validos_para_reviews(enlaces_validos):
    """
    Filtra una lista de enlaces v√°lidos eliminando URLs de streaming
    
    Args:
        enlaces_validos (list): Lista de diccionarios con enlaces
        
    Returns:
        list: Lista filtrada de enlaces v√°lidos para rese√±as
    """
    enlaces_filtrados = []
    
    for enlace in enlaces_validos:
        url = enlace.get('url', '')
        
        if es_url_valida_para_reviews(url):
            enlaces_filtrados.append(enlace)
        else:
            print(f"üö´ Enlace filtrado: {enlace.get('publication', 'Unknown')} - {url}")
    
    print(f"üìä Enlaces despu√©s del filtrado: {len(enlaces_filtrados)}/{len(enlaces_validos)}")
    return enlaces_filtrados



def crear_columna_origen_si_no_existe(cursor):
    """
    A√±ade la columna 'origen' a la tabla feeds si no existe
    
    Args:
        cursor: Cursor de la base de datos
        
    Returns:
        bool: True si se cre√≥ o ya exist√≠a, False si hubo error
    """
    try:
        # Verificar si la columna ya existe
        cursor.execute("PRAGMA table_info(feeds)")
        columns = [column[1] for column in cursor.fetchall()]
        
        if 'origen' not in columns:
            print("A√±adiendo columna 'origen' a la tabla feeds...")
            cursor.execute("ALTER TABLE feeds ADD COLUMN origen TEXT")
            print("‚úì Columna 'origen' a√±adida exitosamente")
        else:
            print("‚úì Columna 'origen' ya existe en la tabla feeds")
        
        return True
    except sqlite3.Error as e:
        print(f"Error al crear columna origen: {e}")
        return False

def verificar_album_ya_procesado(cursor, album_id, origen_fuente):
    """
    Verifica si un √°lbum ya ha sido procesado por una fuente espec√≠fica
    
    Args:
        cursor: Cursor de la base de datos
        album_id (int): ID del √°lbum
        origen_fuente (str): Fuente de origen (review_metacritic, review_aoty, review_anydecentmusic)
        
    Returns:
        bool: True si ya fue procesado, False si no
    """
    try:
        cursor.execute("""
            SELECT COUNT(*) as count FROM feeds 
            WHERE entity_type = 'album' AND entity_id = ? AND origen = ?
        """, (album_id, origen_fuente))
        
        result = cursor.fetchone()
        count = result['count'] if result else 0
        
        if count > 0:
            print(f"   ‚ö†Ô∏è  √Ålbum ID {album_id} ya procesado por {origen_fuente} ({count} rese√±as)")
            return True
        else:
            print(f"   ‚úì √Ålbum ID {album_id} no procesado por {origen_fuente}")
            return False
            
    except sqlite3.Error as e:
        print(f"Error verificando √°lbum procesado: {e}")
        return False



def crear_columna_origen_si_no_existe(cursor):
    """
    A√±ade la columna 'origen' a la tabla feeds si no existe
    
    Args:
        cursor: Cursor de la base de datos
        
    Returns:
        bool: True si se cre√≥ o ya exist√≠a, False si hubo error
    """
    try:
        # Verificar si la columna ya existe
        cursor.execute("PRAGMA table_info(feeds)")
        columns = [column[1] for column in cursor.fetchall()]
        
        if 'origen' not in columns:
            print("A√±adiendo columna 'origen' a la tabla feeds...")
            cursor.execute("ALTER TABLE feeds ADD COLUMN origen TEXT")
            print("‚úì Columna 'origen' a√±adida exitosamente")
        else:
            print("‚úì Columna 'origen' ya existe en la tabla feeds")
        
        return True
    except sqlite3.Error as e:
        print(f"Error al crear columna origen: {e}")
        return False

def verificar_album_ya_procesado(cursor, album_id, origen_fuente):
    """
    Verifica si un √°lbum ya ha sido procesado por una fuente espec√≠fica
    
    Args:
        cursor: Cursor de la base de datos
        album_id (int): ID del √°lbum
        origen_fuente (str): Fuente de origen (review_metacritic, review_aoty, review_anydecentmusic)
        
    Returns:
        bool: True si ya fue procesado, False si no
    """
    try:
        cursor.execute("""
            SELECT COUNT(*) as count FROM feeds 
            WHERE entity_type = 'album' AND entity_id = ? AND origen = ?
        """, (album_id, origen_fuente))
        
        result = cursor.fetchone()
        count = result['count'] if result else 0
        
        if count > 0:
            print(f"   ‚ö†Ô∏è  √Ålbum ID {album_id} ya procesado por {origen_fuente} ({count} rese√±as)")
            return True
        else:
            print(f"   ‚úì √Ålbum ID {album_id} no procesado por {origen_fuente}")
            return False
            
    except sqlite3.Error as e:
        print(f"Error verificando √°lbum procesado: {e}")
        return False

def guardar_feed_con_origen(conn, cursor, entity_type, entity_id, feed_name, post_title, post_url, post_date, content, origen):
    """
    Versi√≥n mejorada de guardar_feed que incluye la columna origen
    
    Args:
        conn: Conexi√≥n a la base de datos
        cursor: Cursor de la base de datos
        entity_type (str): Tipo de entidad ('album')
        entity_id (int): ID de la entidad
        feed_name (str): Nombre del feed (dominio)
        post_title (str): T√≠tulo del post
        post_url (str): URL del post
        post_date (str): Fecha del post (o None)
        content (str): Contenido del post
        origen (str): Origen de la rese√±a (review_metacritic, review_aoty, review_anydecentmusic)
        
    Returns:
        bool: True si se guard√≥ correctamente, False en caso contrario
    """
    try:
        # Verificar si ya existe una entrada para esta URL
        cursor.execute("""
            SELECT id FROM feeds 
            WHERE entity_type = ? AND entity_id = ? AND post_url = ?
        """, (entity_type, entity_id, post_url))
        
        existing = cursor.fetchone()
        
        # Si existe, actualizar
        if existing:
            cursor.execute("""
                UPDATE feeds 
                SET feed_name = ?, post_title = ?, post_date = ?, content = ?, origen = ?, added_date = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (feed_name, post_title, post_date, content, origen, existing['id']))
            print(f"Actualizada rese√±a existente para {post_url} (origen: {origen})")
        else:
            # Si no existe, insertar nuevo
            cursor.execute("""
                INSERT INTO feeds (entity_type, entity_id, feed_name, post_title, post_url, post_date, content, origen, added_date)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (entity_type, entity_id, feed_name, post_title, post_url, post_date, content, origen))
            print(f"Insertada nueva rese√±a para {post_url} (origen: {origen})")
        
        conn.commit()
        return True
    except sqlite3.Error as e:
        print(f"Error al guardar en base de datos: {e}")
        conn.rollback()
        return False

# FUNCIONES MEJORADAS CON force_update Y origen

def buscar_album_en_db_mejorado_con_force_update(conn, cursor, album_id, album_name, artist_name, content_service, 
                                                force_update=False, archivo_errores=None):
    """
    Versi√≥n mejorada de b√∫squeda en AnyDecentMusic con soporte para force_update y origen
    """
    origen = "review_anydecentmusic"
    
    # Verificar si ya fue procesado y force_update est√° desactivado
    if not force_update and verificar_album_ya_procesado(cursor, album_id, origen):
        print(f"   ‚è≠Ô∏è  Saltando √°lbum ID {album_id} - ya procesado por AnyDecentMusic")
        return 0
    
    # Construir la URL de b√∫squeda
    termino_busqueda = artist_name
    url_busqueda = f"http://www.anydecentmusic.com/search-results.aspx?search={urllib.parse.quote(termino_busqueda)}"
    
    print(f"\nüåç Buscando en AnyDecentMusic: {termino_busqueda}")
    print(f"√Ålbum objetivo: {album_name}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        respuesta = requests.get(url_busqueda, headers=headers, timeout=15)
        respuesta.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Error al realizar la petici√≥n: {e}")
        return 0
    
    soup = BeautifulSoup(respuesta.text, 'html.parser')
    resultados = soup.select('form > div > div > div > ul > li > div')
    
    if not resultados:
        print("No se encontraron resultados para la b√∫squeda.")
        return 0
    
    print(f"Se encontraron {len(resultados)} resultados generales.")
    
    album_encontrado = False
    enlaces_guardados = 0
    
    # Verificar cada resultado para encontrar coincidencias
    for idx, resultado in enumerate(resultados, 1):
        artista_elemento = resultado.select_one('a:nth-of-type(2) > h2')
        album_elemento = resultado.select_one('a:nth-of-type(3) > h3')
        
        if artista_elemento and album_elemento:
            nombre_artista = artista_elemento.text.strip()
            nombre_album = album_elemento.text.strip()
            
            print(f"Resultado {idx}: {nombre_artista} - {nombre_album}")
            
            if (artist_name.lower() in nombre_artista.lower() and 
                album_name.lower() in nombre_album.lower()):
                print(f"¬°Coincidencia encontrada! Artista: {nombre_artista}, √Ålbum: {nombre_album}")
                album_encontrado = True
                
                album_url_elemento = resultado.select_one('a:nth-of-type(3)')
                if album_url_elemento and album_url_elemento.has_attr('href'):
                    album_url = album_url_elemento['href']
                    album_url_completa = f"http://www.anydecentmusic.com/{album_url}"
                    print(f"URL del √°lbum: {album_url_completa}")
                    
                    enlaces_validos, enlaces_error = extraer_enlaces_album(album_url_completa)
                    
                    # Filtrar enlaces v√°lidos para rese√±as
                    enlaces_validos = filtrar_enlaces_validos_para_reviews(enlaces_validos)
                    
                    if archivo_errores and enlaces_error:
                        guardar_errores_enlace(archivo_errores, nombre_artista, nombre_album, enlaces_error)
                    
                    if enlaces_validos:
                        print(f"Se encontraron {len(enlaces_validos)} enlaces de rese√±as v√°lidos.")
                        
                        for enlace in enlaces_validos:
                            url = enlace['url']
                            estado = enlace['estado']
                            
                            print(f"Procesando enlace: {url}")
                            
                            feed_name = extraer_dominio(url)
                            post_title = obtener_titulo_pagina(url)
                            
                            # Usar extracci√≥n espec√≠fica por sitio
                            contenido = extraer_contenido_segun_sitio(url, content_service)
                            
                            if contenido:
                                post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                
                                # Usar la nueva funci√≥n con origen
                                guardado = guardar_feed_con_origen(
                                    conn, cursor, 'album', album_id, feed_name, 
                                    post_title, url, post_date, contenido, origen
                                )
                                
                                if guardado:
                                    enlaces_guardados += 1
                                    print(f"Rese√±a guardada: {feed_name} - {post_title}")
                            else:
                                print(f"No se pudo extraer contenido de {url}")
                    else:
                        print("No se encontraron enlaces de rese√±as v√°lidos.")
    
    if not album_encontrado:
        print(f"No se encontr√≥ el √°lbum '{album_name}' para el artista '{artist_name}'.")
    
    return enlaces_guardados

def procesar_reviews_metacritic_mejorado_con_force_update(conn, cursor, album_id, metacritic_url, content_service, force_update=False):
    """
    Versi√≥n mejorada que procesa rese√±as de Metacritic con force_update y origen
    """
    origen = "review_metacritic"
    
    # Verificar si ya fue procesado y force_update est√° desactivado
    if not force_update and verificar_album_ya_procesado(cursor, album_id, origen):
        print(f"   ‚è≠Ô∏è  Saltando rese√±as de Metacritic para √°lbum ID {album_id} - ya procesado")
        return 0
    
    enlaces_validos, enlaces_error = extraer_enlaces_reviews_metacritic(metacritic_url)
    
    # Filtrar enlaces v√°lidos para rese√±as
    enlaces_validos = filtrar_enlaces_validos_para_reviews(enlaces_validos)
    
    if enlaces_error:
        print(f"Se encontraron {len(enlaces_error)} enlaces con errores en Metacritic")
    
    reviews_guardadas = 0
    
    for enlace in enlaces_validos:
        url = enlace['url']
        publication = enlace['publication']
        score = enlace.get('score')
        
        print(f"Procesando rese√±a de {publication}: {url}")
        
        try:
            # Usar la nueva funci√≥n de extracci√≥n espec√≠fica por sitio
            contenido = extraer_contenido_segun_sitio(url, content_service)
            
            if contenido:
                # Crear t√≠tulo para la rese√±a
                post_title = f"{publication} Review"
                if score:
                    post_title += f" (Score: {score})"
                
                # Fecha actual
                post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # Guardar en la tabla feeds con origen
                feed_name = extraer_dominio(url)
                
                guardado = guardar_feed_con_origen(
                    conn, cursor, 'album', album_id, feed_name,
                    post_title, url, post_date, contenido, origen
                )
                
                if guardado:
                    reviews_guardadas += 1
                    print(f"‚úì Rese√±a guardada: {publication} - {post_title}")
                else:
                    print(f"‚úó Error guardando rese√±a de {publication}")
            else:
                print(f"‚úó No se pudo extraer contenido de {publication}: {url}")
                
        except Exception as e:
            print(f"‚úó Error procesando rese√±a de {publication}: {e}")
            continue
    
    print(f"Total de rese√±as guardadas desde Metacritic: {reviews_guardadas}")
    return reviews_guardadas

def procesar_reviews_aoty_mejorado_con_force_update(conn, cursor, album_id, aoty_url, content_service, force_update=False):
    """
    Versi√≥n mejorada que procesa rese√±as de AOTY con force_update y origen
    """
    origen = "review_aoty"
    
    # Verificar si ya fue procesado y force_update est√° desactivado
    if not force_update and verificar_album_ya_procesado(cursor, album_id, origen):
        print(f"   ‚è≠Ô∏è  Saltando rese√±as de AOTY para √°lbum ID {album_id} - ya procesado")
        return 0
    
    enlaces_validos, enlaces_error = extraer_enlaces_reviews_aoty(aoty_url)
    
    # Filtrar enlaces v√°lidos para rese√±as
    enlaces_validos = filtrar_enlaces_validos_para_reviews(enlaces_validos)
    
    if enlaces_error:
        print(f"Se encontraron {len(enlaces_error)} enlaces con errores en AOTY")
    
    reviews_guardadas = 0
    
    for enlace in enlaces_validos:
        url = enlace['url']
        publication = enlace['publication']
        
        print(f"Procesando rese√±a de {publication}: {url}")
        
        try:
            # Usar la nueva funci√≥n de extracci√≥n espec√≠fica por sitio
            contenido = extraer_contenido_segun_sitio(url, content_service)
            
            if contenido:
                # Crear t√≠tulo para la rese√±a
                post_title = f"{publication} Review (via AOTY)"
                
                # Fecha actual
                post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # Guardar en la tabla feeds con origen
                feed_name = extraer_dominio(url)
                
                guardado = guardar_feed_con_origen(
                    conn, cursor, 'album', album_id, feed_name,
                    post_title, url, post_date, contenido, origen
                )
                
                if guardado:
                    reviews_guardadas += 1
                    print(f"‚úì Rese√±a guardada: {publication} - {post_title}")
                else:
                    print(f"‚úó Error guardando rese√±a de {publication}")
            else:
                print(f"‚úó No se pudo extraer contenido de {publication}: {url}")
                
        except Exception as e:
            print(f"‚úó Error procesando rese√±a de {publication}: {e}")
            continue
    
    print(f"Total de rese√±as guardadas desde AOTY: {reviews_guardadas}")
    return reviews_guardadas

def buscar_album_en_db_completo_con_force_update(conn, cursor, album_id, album_name, artist_name, content_service, 
                                               include_metacritic=True, include_aoty=True, 
                                               force_update=False, archivo_errores=None,
                                               google_api_key=None, google_cx=None):
    """
    Versi√≥n completa con force_update y origen para todas las fuentes
    """
    enlaces_guardados = 0
    
    print(f"\n{'='*60}")
    print(f"üéµ PROCESANDO √ÅLBUM (force_update={force_update})")
    print(f"   üé§ {artist_name} - üíø {album_name}")
    print(f"{'='*60}")
    
    # 1. B√∫squeda mejorada en AnyDecentMusic
    try:
        print(f"\nüåç Procesando AnyDecentMusic...")
        enlaces_guardados += buscar_album_en_db_mejorado_con_force_update(
            conn, cursor, album_id, album_name, artist_name, content_service, 
            force_update, archivo_errores
        )
    except Exception as e:
        print(f"Error en b√∫squeda AnyDecentMusic: {e}")
    
    # 2. B√∫squeda completa en Metacritic
    if include_metacritic:
        print(f"\nüéØ Procesando Metacritic...")
        try:
            # Verificar si ya fue procesado por Metacritic (datos b√°sicos)
            if not force_update:
                cursor.execute("SELECT metascore FROM album_metacritic WHERE album_id = ?", (album_id,))
                if cursor.fetchone():
                    print(f"   ‚è≠Ô∏è  Saltando datos de Metacritic para √°lbum ID {album_id} - ya procesado")
                else:
                    # Solo buscar y guardar datos b√°sicos si no existen
                    metacritic_url = buscar_album_metacritic_mejorado(artist_name, album_name)
                    
                    if metacritic_url:
                        print(f"‚úì √Ålbum encontrado en Metacritic: {metacritic_url}")
                        
                        # Extraer y guardar datos b√°sicos de Metacritic
                        datos_metacritic = extraer_datos_metacritic_mejorado(metacritic_url)
                        
                        if datos_metacritic:
                            if guardar_datos_metacritic(conn, cursor, album_id, datos_metacritic):
                                print(f"‚úì Datos b√°sicos de Metacritic guardados")
                            
                            # Guardar resumen como entrada en feeds con origen
                            if datos_metacritic['metascore'] is not None:
                                post_title = f"Metacritic Summary - {artist_name} - {album_name} (Score: {datos_metacritic['metascore']})"
                                content = f"Metacritic Score: {datos_metacritic['metascore']}/100\n"
                                content += f"Based on {datos_metacritic['num_critics']} critic reviews\n"
                                content += f"Distribution: +{datos_metacritic['positive_reviews']} ¬±{datos_metacritic['mixed_reviews']} -{datos_metacritic['negative_reviews']}"
                                
                                post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                
                                if guardar_feed_con_origen(conn, cursor, 'album', album_id, 'metacritic.com', 
                                                      post_title, metacritic_url, post_date, content, 'review_metacritic'):
                                    enlaces_guardados += 1
                        
                        # Procesar rese√±as individuales de Metacritic con force_update
                        print(f"üîç Buscando rese√±as individuales en Metacritic...")
                        reviews_procesadas = procesar_reviews_metacritic_mejorado_con_force_update(
                            conn, cursor, album_id, metacritic_url, content_service, force_update
                        )
                        enlaces_guardados += reviews_procesadas
                    else:
                        print(f"‚úó No se encontr√≥ el √°lbum en Metacritic")
            else:
                # Si force_update=True, procesar todo nuevamente
                metacritic_url = buscar_album_metacritic_mejorado(artist_name, album_name)
                
                if metacritic_url:
                    print(f"‚úì √Ålbum encontrado en Metacritic: {metacritic_url}")
                    
                    # Extraer y guardar datos b√°sicos de Metacritic
                    datos_metacritic = extraer_datos_metacritic_mejorado(metacritic_url)
                    
                    if datos_metacritic:
                        if guardar_datos_metacritic(conn, cursor, album_id, datos_metacritic):
                            print(f"‚úì Datos b√°sicos de Metacritic guardados")
                        
                        # Guardar resumen como entrada en feeds con origen
                        if datos_metacritic['metascore'] is not None:
                            post_title = f"Metacritic Summary - {artist_name} - {album_name} (Score: {datos_metacritic['metascore']})"
                            content = f"Metacritic Score: {datos_metacritic['metascore']}/100\n"
                            content += f"Based on {datos_metacritic['num_critics']} critic reviews\n"
                            content += f"Distribution: +{datos_metacritic['positive_reviews']} ¬±{datos_metacritic['mixed_reviews']} -{datos_metacritic['negative_reviews']}"
                            
                            post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            
                            if guardar_feed_con_origen(conn, cursor, 'album', album_id, 'metacritic.com', 
                                                  post_title, metacritic_url, post_date, content, 'review_metacritic'):
                                enlaces_guardados += 1
                    
                    # Procesar rese√±as individuales de Metacritic
                    print(f"üîç Buscando rese√±as individuales en Metacritic...")
                    reviews_procesadas = procesar_reviews_metacritic_mejorado_con_force_update(
                        conn, cursor, album_id, metacritic_url, content_service, force_update
                    )
                    enlaces_guardados += reviews_procesadas
                else:
                    print(f"‚úó No se encontr√≥ el √°lbum en Metacritic")
                    
        except Exception as e:
            print(f"Error en b√∫squeda Metacritic: {e}")
    
    # 3. B√∫squeda completa en Album of the Year (AOTY)
    if include_aoty:
        print(f"\nüèÜ Procesando Album of the Year...")
        try:
            # Verificar si ya fue procesado por AOTY (datos b√°sicos)
            if not force_update:
                cursor.execute("SELECT user_score FROM album_aoty WHERE album_id = ?", (album_id,))
                if cursor.fetchone():
                    print(f"   ‚è≠Ô∏è  Saltando datos de AOTY para √°lbum ID {album_id} - ya procesado")
                else:
                    # Solo buscar y guardar datos b√°sicos si no existen
                    aoty_url = buscar_album_aoty(artist_name, album_name, google_api_key, google_cx)
                    
                    if aoty_url:
                        print(f"‚úì √Ålbum encontrado en AOTY: {aoty_url}")
                        
                        # Extraer y guardar datos b√°sicos de AOTY
                        datos_aoty = extraer_datos_aoty(aoty_url)
                        
                        if datos_aoty:
                            if guardar_datos_aoty(conn, cursor, album_id, datos_aoty):
                                print(f"‚úì Datos b√°sicos de AOTY guardados")
                            
                            # Guardar resumen como entrada en feeds con origen
                            if datos_aoty['user_score'] is not None or datos_aoty['critic_score'] is not None:
                                scores_text = []
                                if datos_aoty['user_score']:
                                    scores_text.append(f"User: {datos_aoty['user_score']}")
                                if datos_aoty['critic_score']:
                                    scores_text.append(f"Critic: {datos_aoty['critic_score']}")
                                
                                post_title = f"AOTY Summary - {artist_name} - {album_name} ({'/'.join(scores_text)})"
                                content = f"Album of the Year Scores:\n"
                                if datos_aoty['user_score']:
                                    content += f"User Score: {datos_aoty['user_score']}/100 (based on {datos_aoty['num_user_ratings']} ratings)\n"
                                if datos_aoty['critic_score']:
                                    content += f"Critic Score: {datos_aoty['critic_score']}/100 (based on {datos_aoty['num_critic_ratings']} reviews)\n"
                                
                                post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                
                                if guardar_feed_con_origen(conn, cursor, 'album', album_id, 'albumoftheyear.org', 
                                                      post_title, aoty_url, post_date, content, 'review_aoty'):
                                    enlaces_guardados += 1
                        
                        # Procesar rese√±as individuales de AOTY con force_update
                        print(f"üîç Buscando rese√±as individuales en AOTY...")
                        reviews_procesadas = procesar_reviews_aoty_mejorado_con_force_update(
                            conn, cursor, album_id, aoty_url, content_service, force_update
                        )
                        enlaces_guardados += reviews_procesadas
                    else:
                        print(f"‚úó No se encontr√≥ el √°lbum en Album of the Year")
            else:
                # Si force_update=True, procesar todo nuevamente
                aoty_url = buscar_album_aoty(artist_name, album_name, google_api_key, google_cx)
                
                if aoty_url:
                    print(f"‚úì √Ålbum encontrado en AOTY: {aoty_url}")
                    
                    # Extraer y guardar datos b√°sicos de AOTY
                    datos_aoty = extraer_datos_aoty(aoty_url)
                    
                    if datos_aoty:
                        if guardar_datos_aoty(conn, cursor, album_id, datos_aoty):
                            print(f"‚úì Datos b√°sicos de AOTY guardados")
                        
                        # Guardar resumen como entrada en feeds con origen
                        if datos_aoty['user_score'] is not None or datos_aoty['critic_score'] is not None:
                            scores_text = []
                            if datos_aoty['user_score']:
                                scores_text.append(f"User: {datos_aoty['user_score']}")
                            if datos_aoty['critic_score']:
                                scores_text.append(f"Critic: {datos_aoty['critic_score']}")
                            
                            post_title = f"AOTY Summary - {artist_name} - {album_name} ({'/'.join(scores_text)})"
                            content = f"Album of the Year Scores:\n"
                            if datos_aoty['user_score']:
                                content += f"User Score: {datos_aoty['user_score']}/100 (based on {datos_aoty['num_user_ratings']} ratings)\n"
                            if datos_aoty['critic_score']:
                                content += f"Critic Score: {datos_aoty['critic_score']}/100 (based on {datos_aoty['num_critic_ratings']} reviews)\n"
                            
                            post_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            
                            if guardar_feed_con_origen(conn, cursor, 'album', album_id, 'albumoftheyear.org', 
                                                  post_title, aoty_url, post_date, content, 'review_aoty'):
                                enlaces_guardados += 1
                    
                    # Procesar rese√±as individuales de AOTY
                    print(f"üîç Buscando rese√±as individuales en AOTY...")
                    reviews_procesadas = procesar_reviews_aoty_mejorado_con_force_update(
                        conn, cursor, album_id, aoty_url, content_service, force_update
                    )
                    enlaces_guardados += reviews_procesadas
                else:
                    print(f"‚úó No se encontr√≥ el √°lbum en Album of the Year")
                    
        except Exception as e:
            print(f"Error en b√∫squeda AOTY: {e}")
    
    return enlaces_guardados

def procesar_albums_con_force_update_y_origen(db_path, content_service, include_metacritic=True, 
                                            include_aoty=True, archive_errores=None, 
                                            inicio_id=0, lote=50, pausa=2, 
                                            force_update=False,
                                            google_api_key=None, google_cx=None):
    """
    Funci√≥n principal con soporte completo para force_update y columna origen
    """
    conn, cursor = conectar_db(db_path)
    
    # Crear columna origen si no existe
    crear_columna_origen_si_no_existe(cursor)
    conn.commit()
    
    # Crear tablas si no existen
    if include_metacritic:
        crear_tabla_metacritic(cursor)
    if include_aoty:
        crear_tabla_aoty(cursor)
    conn.commit()
    
    # Obtener todos los √°lbumes
    todos_albums = obtener_albums(cursor)
    albums_a_procesar = [a for a in todos_albums if a['id'] >= inicio_id]
    
    print(f"\n{'='*80}")
    print(f"üöÄ PROCESAMIENTO CON force_update Y COLUMNA origen")
    print(f"{'='*80}")
    print(f"üìä √Ålbumes a procesar: {len(albums_a_procesar)} (desde ID {inicio_id})")
    print(f"üîÑ Force update: {'ACTIVADO' if force_update else 'DESACTIVADO'}")
    
    if force_update:
        print(f"   ‚Üí Se procesar√°n TODOS los √°lbumes nuevamente")
    else:
        print(f"   ‚Üí Se saltar√°n √°lbumes ya procesados por cada fuente")
    
    fuentes_activas = ["AnyDecentMusic"]
    if include_metacritic:
        fuentes_activas.append("Metacritic")
    if include_aoty:
        fuentes_activas.append("Album of the Year")
    
    print(f"üìÑ Fuentes activas: {', '.join(fuentes_activas)}")
    print(f"‚öôÔ∏è  Servicio de contenido: {content_service}")
    print(f"üè∑Ô∏è  Columna origen: ACTIVADA (review_anydecentmusic, review_metacritic, review_aoty)")
    print(f"{'='*80}")
    
    # Estad√≠sticas
    albums_procesados = 0
    resenas_totales = 0
    albums_saltados = 0
    metacritic_encontrados = 0
    aoty_encontrados = 0
    anydecentmusic_encontrados = 0
    
    # Procesar por lotes
    for i in range(0, len(albums_a_procesar), lote):
        lote_actual = albums_a_procesar[i:i+lote]
        print(f"\n{'='*60}")
        print(f"üì¶ PROCESANDO LOTE {i//lote + 1}/{(len(albums_a_procesar)-1)//lote + 1} ({len(lote_actual)} √°lbumes)")
        print(f"{'='*60}")
        
        for album in lote_actual:
            print(f"\nüéµ √ÅLBUM {albums_procesados + 1}/{len(albums_a_procesar)} - ID {album['id']}")
            print(f"   üé§ {album['artist']} - üíø {album['name']}")
            
            # Verificar si el √°lbum debe ser saltado completamente
            album_saltado = False
            if not force_update:
                # Verificar si ya fue procesado por TODAS las fuentes activas
                fuentes_procesadas = 0
                
                if verificar_album_ya_procesado(cursor, album['id'], 'review_anydecentmusic'):
                    fuentes_procesadas += 1
                
                if include_metacritic and verificar_album_ya_procesado(cursor, album['id'], 'review_metacritic'):
                    fuentes_procesadas += 1
                
                if include_aoty and verificar_album_ya_procesado(cursor, album['id'], 'review_aoty'):
                    fuentes_procesadas += 1
                
                total_fuentes = 1 + (1 if include_metacritic else 0) + (1 if include_aoty else 0)
                
                if fuentes_procesadas == total_fuentes:
                    print(f"   ‚è≠Ô∏è  √ÅLBUM COMPLETAMENTE PROCESADO - Saltando")
                    albums_saltados += 1
                    album_saltado = True
            
            if not album_saltado:
                # Buscar rese√±as para este √°lbum con force_update
                resenas = buscar_album_en_db_completo_con_force_update(
                    conn, cursor, album['id'], album['name'], album['artist'], 
                    content_service, include_metacritic, include_aoty, 
                    force_update, archive_errores, google_api_key, google_cx 
                )
                
                # Actualizar estad√≠sticas
                resenas_totales += resenas
                print(f"   üìà Rese√±as encontradas para este √°lbum: {resenas}")
            
            # Verificar fuentes encontradas para estad√≠sticas
            if include_metacritic:
                cursor.execute("SELECT metascore FROM album_metacritic WHERE album_id = ?", (album['id'],))
                if cursor.fetchone():
                    metacritic_encontrados += 1
            
            if include_aoty:
                cursor.execute("SELECT user_score FROM album_aoty WHERE album_id = ?", (album['id'],))
                if cursor.fetchone():
                    aoty_encontrados += 1
            
            # Contar rese√±as de AnyDecentMusic
            cursor.execute("""
                SELECT COUNT(*) as count FROM feeds 
                WHERE entity_type = 'album' AND entity_id = ? 
                AND origen = 'review_anydecentmusic'
            """, (album['id'],))
            if cursor.fetchone()['count'] > 0:
                anydecentmusic_encontrados += 1
            
            albums_procesados += 1
            
            # Pausa para evitar sobrecargar los servidores
            if albums_procesados < len(albums_a_procesar):
                print(f"   ‚è≥ Esperando {pausa} segundos...")
                time.sleep(pausa)
        
        # Resumen del lote
        print(f"\n{'='*40}")
        print(f"üìä RESUMEN LOTE {i//lote + 1}")
        print(f"{'='*40}")
        print(f"‚úÖ √Ålbumes procesados: {albums_procesados}/{len(albums_a_procesar)}")
        print(f"‚è≠Ô∏è  √Ålbumes saltados: {albums_saltados}")
        print(f"üìÑ Rese√±as totales: {resenas_totales}")
        print(f"üåç Encontrados en AnyDecentMusic: {anydecentmusic_encontrados}")
        if include_metacritic:
            print(f"üéØ Encontrados en Metacritic: {metacritic_encontrados}")
        if include_aoty:
            print(f"üèÜ Encontrados en Album of the Year: {aoty_encontrados}")
        print(f"{'='*40}")
    
    # Cerrar conexi√≥n
    conn.close()
    
    return albums_procesados, resenas_totales, albums_saltados, metacritic_encontrados, aoty_encontrados, anydecentmusic_encontrados


def generar_variaciones_nombre(nombre):
    """
    Versi√≥n corregida que genera variaciones √∫nicas de un nombre para b√∫squeda
    
    Args:
        nombre (str): Nombre original
        
    Returns:
        list: Lista de variaciones √∫nicas ordenadas por relevancia
    """
    variaciones = []  # Usar lista para mantener orden de relevancia
    variaciones_set = set()  # Set para evitar duplicados
    
    nombre_lower = nombre.strip().lower()
    
    # 1. Original (m√°s relevante)
    if nombre_lower not in variaciones_set:
        variaciones.append(nombre_lower)
        variaciones_set.add(nombre_lower)
    
    # 2. Sin art√≠culos al inicio (muy relevante)
    sin_articulos = re.sub(r'^(the|a|an)\s+', '', nombre, flags=re.IGNORECASE).strip().lower()
    if sin_articulos and sin_articulos not in variaciones_set:
        variaciones.append(sin_articulos)
        variaciones_set.add(sin_articulos)
    
    # 3. Sin par√©ntesis (relevante)
    sin_parentesis = re.sub(r'\s*\([^)]*\)', '', nombre).strip().lower()
    if sin_parentesis and sin_parentesis not in variaciones_set:
        variaciones.append(sin_parentesis)
        variaciones_set.add(sin_parentesis)
    
    # 4. Sin "featuring", "feat.", "ft." (relevante)
    sin_featuring = re.sub(r'\s+(featuring|feat\.?|ft\.?)\s+.*$', '', nombre, flags=re.IGNORECASE).strip().lower()
    if sin_featuring and sin_featuring not in variaciones_set:
        variaciones.append(sin_featuring)
        variaciones_set.add(sin_featuring)
    
    # 5. Sin corchetes (menos relevante)
    sin_corchetes = re.sub(r'\s*\[[^\]]*\]', '', nombre).strip().lower()
    if sin_corchetes and sin_corchetes not in variaciones_set:
        variaciones.append(sin_corchetes)
        variaciones_set.add(sin_corchetes)
    
    # Limitar a m√°ximo 4 variaciones para evitar demasiadas combinaciones
    return variaciones[:4]

def buscar_album_metacritic_mejorado_v2(artist_name, album_name):
    """
    Versi√≥n mejorada que evita repeticiones y usa DuckDuckGo como m√©todo principal
    
    Args:
        artist_name (str): Nombre del artista
        album_name (str): Nombre del √°lbum
        
    Returns:
        str or None: URL del √°lbum en Metacritic si se encuentra
    """
    print(f"Buscando en Metacritic: {artist_name} - {album_name}")
    
    # M√©todo 1: B√∫squeda con DuckDuckGo (m√°s efectivo)
    metacritic_url = buscar_metacritic_con_duckduckgo(artist_name, album_name)
    if metacritic_url:
        return metacritic_url
    
    # M√©todo 2: URLs directas con variaciones (sin repeticiones)
    print("Probando URLs directas con variaciones...")
    
    # Generar variaciones √∫nicas
    variaciones_album = generar_variaciones_nombre(album_name)
    variaciones_artista = generar_variaciones_nombre(artist_name)
    
    print(f"Variaciones de √°lbum ({len(variaciones_album)}): {variaciones_album}")
    print(f"Variaciones de artista ({len(variaciones_artista)}): {variaciones_artista}")
    
    # Probar combinaciones √∫nicas (m√°ximo 6 intentos)
    combinaciones_probadas = set()
    intentos = 0
    max_intentos = 6
    
    for album_var in variaciones_album:
        for artist_var in variaciones_artista:
            if intentos >= max_intentos:
                break
                
            # Crear clave √∫nica para evitar repeticiones
            clave_combinacion = f"{album_var}|{artist_var}"
            if clave_combinacion in combinaciones_probadas:
                continue
                
            combinaciones_probadas.add(clave_combinacion)
            intentos += 1
            
            url_directa = construir_url_metacritic_directa(artist_var, album_var)
            print(f"Intento {intentos}: {url_directa}")
            
            if verificar_url_metacritic(url_directa):
                print(f"‚úì Encontrado en URL directa: {url_directa}")
                return url_directa
        
        if intentos >= max_intentos:
            break
    
    # M√©todo 3: Fallback con buscador interno de Metacritic
    print("Probando buscador interno de Metacritic como √∫ltimo recurso...")
    return buscar_album_metacritic_fallback(artist_name, album_name)


def generar_variaciones_nombre(nombre):
    """
    Versi√≥n corregida que genera variaciones √∫nicas de un nombre para b√∫squeda
    
    Args:
        nombre (str): Nombre original
        
    Returns:
        list: Lista de variaciones √∫nicas ordenadas por relevancia
    """
    variaciones = []  # Usar lista para mantener orden de relevancia
    variaciones_set = set()  # Set para evitar duplicados
    
    nombre_lower = nombre.strip().lower()
    
    # 1. Original (m√°s relevante)
    if nombre_lower not in variaciones_set:
        variaciones.append(nombre_lower)
        variaciones_set.add(nombre_lower)
    
    # 2. Sin art√≠culos al inicio (muy relevante)
    sin_articulos = re.sub(r'^(the|a|an)\s+', '', nombre, flags=re.IGNORECASE).strip().lower()
    if sin_articulos and sin_articulos not in variaciones_set:
        variaciones.append(sin_articulos)
        variaciones_set.add(sin_articulos)
    
    # 3. Sin par√©ntesis (relevante)
    sin_parentesis = re.sub(r'\s*\([^)]*\)', '', nombre).strip().lower()
    if sin_parentesis and sin_parentesis not in variaciones_set:
        variaciones.append(sin_parentesis)
        variaciones_set.add(sin_parentesis)
    
    # 4. Sin "featuring", "feat.", "ft." (relevante)
    sin_featuring = re.sub(r'\s+(featuring|feat\.?|ft\.?)\s+.*$', '', nombre, flags=re.IGNORECASE).strip().lower()
    if sin_featuring and sin_featuring not in variaciones_set:
        variaciones.append(sin_featuring)
        variaciones_set.add(sin_featuring)
    
    # 5. Sin corchetes (menos relevante)
    sin_corchetes = re.sub(r'\s*\[[^\]]*\]', '', nombre).strip().lower()
    if sin_corchetes and sin_corchetes not in variaciones_set:
        variaciones.append(sin_corchetes)
        variaciones_set.add(sin_corchetes)
    
    # Limitar a m√°ximo 4 variaciones para evitar demasiadas combinaciones
    return variaciones[:4]



def buscar_metacritic_con_duckduckgo(artist_name, album_name):
    """
    Busca en Metacritic usando DuckDuckGo con medidas anti-detecci√≥n
    """
    from urllib.parse import quote_plus
    import time
    import random
    
    # Lista de user agents reales
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0'
    ]
    
    # Queries limitadas
    queries = [
        f'site:metacritic.com "{artist_name}" "{album_name}" music',
        f'site:metacritic.com {artist_name} {album_name} album'
    ]
    
    for i, query in enumerate(queries, 1):
        try:
            # Delay aleatorio antes de cada b√∫squeda
            if i > 1:
                delay = random.uniform(3, 8)  # Entre 3 y 8 segundos
                print(f"Esperando {delay:.1f} segundos antes de la siguiente b√∫squeda...")
                time.sleep(delay)
            
            # Rotar user agent
            headers = {
                'User-Agent': random.choice(user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,es;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Cache-Control': 'max-age=0',
                'Pragma': 'no-cache'
            }
            
            print(f"B√∫squeda DuckDuckGo {i}/{len(queries)}: {query}")
            
            search_url = f"https://duckduckgo.com/html/?q={quote_plus(query)}"
            
            # Usar sesi√≥n para mantener cookies
            session = requests.Session()
            session.headers.update(headers)
            
            response = session.get(search_url, timeout=15)
            response.raise_for_status()
            
            # Verificar si hay captcha
            if 'captcha' in response.text.lower() or 'robot' in response.text.lower():
                print("‚ö†Ô∏è Detectado posible captcha, saltando b√∫squeda DuckDuckGo")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Buscar enlaces
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href']
                
                if '/l/?uddg=' in href:
                    try:
                        import urllib.parse
                        href = urllib.parse.unquote(href.split('uddg=')[1].split('&')[0])
                    except:
                        continue
                
                if ('metacritic.com' in href and '/music/' in href and href.startswith('http')):
                    clean_url = href.split('?')[0].split('#')[0]
                    print(f"Candidato encontrado: {clean_url}")
                    
                    # Peque√±o delay antes de verificar
                    time.sleep(random.uniform(1, 2))
                    
                    if verificar_url_metacritic(clean_url):
                        print(f"‚úì Encontrado con DuckDuckGo: {clean_url}")
                        return clean_url
                
        except Exception as e:
            print(f"Error en b√∫squeda DuckDuckGo '{query}': {e}")
            continue
    
    return None

def buscar_metacritic_con_google_scraping(artist_name, album_name):
    """
    Busca usando Google Search (scraping) como alternativa a DuckDuckGo
    """
    from urllib.parse import quote_plus
    import time
    import random
    
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    
    query = f'site:metacritic.com "{artist_name}" "{album_name}" music album'
    
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    try:
        # Usar google.com/search
        search_url = f"https://www.google.com/search?q={quote_plus(query)}"
        
        print(f"Buscando con Google: {query}")
        
        response = requests.get(search_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        # Verificar si hay captcha
        if 'captcha' in response.text.lower() or '/sorry/' in response.url:
            print("‚ö†Ô∏è Google detect√≥ tr√°fico inusual")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Buscar enlaces en los resultados
        for link in soup.find_all('a', href=True):
            href = link['href']
            
            # Extraer URL real de Google
            if '/url?q=' in href:
                try:
                    import urllib.parse
                    real_url = urllib.parse.unquote(href.split('/url?q=')[1].split('&')[0])
                    
                    if 'metacritic.com' in real_url and '/music/' in real_url:
                        print(f"Candidato encontrado: {real_url}")
                        
                        # Delay antes de verificar
                        time.sleep(random.uniform(1, 2))
                        
                        if verificar_url_metacritic(real_url):
                            print(f"‚úì Encontrado con Google: {real_url}")
                            return real_url
                except:
                    continue
        
        return None
        
    except Exception as e:
        print(f"Error en b√∫squeda Google: {e}")
        return None



def buscar_album_aoty_mejorado_v2(artist_name, album_name, google_api_key=None, google_cx=None):
    """
    Versi√≥n mejorada que usa DuckDuckGo como m√©todo principal para AOTY
    
    Args:
        artist_name (str): Nombre del artista
        album_name (str): Nombre del √°lbum
        google_api_key (str): API key de Google (opcional)
        google_cx (str): Custom Search Engine ID (opcional)
        
    Returns:
        str or None: URL del √°lbum en AOTY si se encuentra
    """
    print(f"Buscando en Album of the Year: {artist_name} - {album_name}")
    
    # M√©todo 1: DuckDuckGo (m√°s efectivo que Google API)
    print("Usando DuckDuckGo como m√©todo principal...")
    aoty_url = buscar_aoty_con_duckduckgo_v2(artist_name, album_name)
    if aoty_url:
        return aoty_url
    
    # M√©todo 2: Google API (si est√° disponible)
    if google_api_key and google_cx:
        try:
            print("Probando con Google API...")
            aoty_url = buscar_aoty_con_google_api(artist_name, album_name, google_api_key, google_cx)
            if aoty_url:
                return aoty_url
        except Exception as e:
            print(f"Error con Google API: {e}")
    
    # M√©todo 3: B√∫squeda directa en AOTY (fallback)
    print("Usando b√∫squeda directa como √∫ltimo recurso...")
    return buscar_album_aoty_directo_mejorado(artist_name, album_name)

def buscar_aoty_con_duckduckgo_v2(artist_name, album_name):
    """
    Versi√≥n mejorada de b√∫squeda en AOTY con DuckDuckGo
    
    Args:
        artist_name (str): Nombre del artista
        album_name (str): Nombre del √°lbum
        
    Returns:
        str or None: URL del √°lbum en AOTY si se encuentra
    """
    from urllib.parse import quote_plus
    import time
    
    # Generar variaciones limitadas para b√∫squeda m√°s efectiva
    variaciones_album = generar_variaciones_nombre(album_name)[:2]  # Solo las 2 m√°s relevantes
    variaciones_artista = generar_variaciones_nombre(artist_name)[:2]  # Solo las 2 m√°s relevantes
    
    # Queries m√°s espec√≠ficas para AOTY (limitadas)
    queries = []
    
    # Usar las mejores variaciones para crear queries
    for album_var in variaciones_album:
        for artist_var in variaciones_artista:
            queries.append(f'site:albumoftheyear.org "{artist_var}" "{album_var}"')
    
    # Agregar queries adicionales m√°s generales
    queries.extend([
        f'site:albumoftheyear.org {artist_name} {album_name} album',
        f'albumoftheyear "{artist_name}" "{album_name}"'
    ])
    
    # Limitar el n√∫mero total de queries a 6
    queries = queries[:6]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive'
    }
    
    for i, query in enumerate(queries, 1):
        try:
            print(f"B√∫squeda AOTY DuckDuckGo {i}/{len(queries)}: {query}")
            
            search_url = f"https://duckduckgo.com/html/?q={quote_plus(query)}"
            
            response = requests.get(search_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Buscar enlaces en los resultados
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href']
                
                # Limpiar URL de DuckDuckGo
                if '/l/?uddg=' in href:
                    try:
                        import urllib.parse
                        href = urllib.parse.unquote(href.split('uddg=')[1].split('&')[0])
                    except:
                        continue
                
                # Verificar si es una URL de AOTY v√°lida
                if ('albumoftheyear.org' in href and 
                    '/album/' in href and 
                    href.startswith('http')):
                    
                    # Limpiar par√°metros adicionales
                    clean_url = href.split('?')[0].split('#')[0]
                    
                    print(f"Candidato AOTY encontrado: {clean_url}")
                    
                    if verificar_url_aoty_mejorado(clean_url):
                        print(f"‚úì Encontrado AOTY con DuckDuckGo: {clean_url}")
                        return clean_url
            
            # Pausa entre consultas
            if i < len(queries):
                time.sleep(2)
                
        except Exception as e:
            print(f"Error en b√∫squeda AOTY DuckDuckGo '{query}': {e}")
            continue
    
    print("No se encontr√≥ resultado v√°lido en AOTY con DuckDuckGo")
    return None

def buscar_album_aoty_directo_mejorado(artist_name, album_name):
    """
    Versi√≥n mejorada de b√∫squeda directa en AOTY con mejor manejo de errores
    
    Args:
        artist_name (str): Nombre del artista
        album_name (str): Nombre del √°lbum
        
    Returns:
        str or None: URL del √°lbum en AOTY si se encuentra
    """
    from urllib.parse import quote_plus
    
    print(f"B√∫squeda directa en AOTY...")
    
    # Usar solo las mejores variaciones para evitar spam
    variaciones_album = generar_variaciones_nombre(album_name)[:2]
    variaciones_artista = generar_variaciones_nombre(artist_name)[:2]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9'
    }
    
    # Probar diferentes queries de b√∫squeda
    search_queries = []
    
    for album_var in variaciones_album:
        for artist_var in variaciones_artista:
            query = f"{artist_var} {album_var}".strip()
            if query not in search_queries:
                search_queries.append(query)
    
    # Limitar a 4 queries m√°ximo
    search_queries = search_queries[:4]
    
    for i, query in enumerate(search_queries, 1):
        try:
            search_url = f"https://www.albumoftheyear.org/search/albums/?q={quote_plus(query)}"
            print(f"B√∫squeda directa AOTY {i}/{len(search_queries)}: {search_url}")
            
            response = requests.get(search_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # M√∫ltiples selectores para encontrar enlaces de √°lbumes
            selectors_to_try = [
                'div.albumBlock a[href*="/album/"]',
                'a[href*="/album/"]',
                '.searchResult a[href*="/album/"]',
                '.albumEntry a[href*="/album/"]'
            ]
            
            album_links = []
            for selector in selectors_to_try:
                links = soup.select(selector)
                if links:
                    album_links = links[:5]  # Solo los primeros 5
                    print(f"Encontrados {len(links)} enlaces con selector: {selector}")
                    break
            
            # Verificar cada resultado
            for link in album_links:
                href = link.get('href', '')
                
                if href.startswith('/'):
                    result_url = f"https://www.albumoftheyear.org{href}"
                else:
                    result_url = href
                
                if verificar_url_aoty_mejorado(result_url):
                    print(f"‚úì Primer resultado v√°lido en b√∫squeda directa AOTY: {result_url}")
                    return result_url
            
            # Pausa entre queries
            if i < len(search_queries):
                time.sleep(1)
                
        except Exception as e:
            print(f"Error en b√∫squeda directa AOTY query '{query}': {e}")
            continue
    
    return None


def verificar_url_aoty_mejorado(url):
    """
    Versi√≥n mejorada de verificaci√≥n para AOTY
    
    Args:
        url (str): URL a verificar
        
    Returns:
        bool: True si la URL existe y es v√°lida
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    try:
        response = requests.head(url, headers=headers, timeout=10, allow_redirects=True)
        
        # Aceptar varios c√≥digos de estado v√°lidos
        if response.status_code in [200, 301, 302]:
            # Verificar que la URL final siga siendo de AOTY
            if 'albumoftheyear.org' in response.url:
                return True
        
        return False
    except Exception as e:
        print(f"Error verificando URL AOTY {url}: {e}")
        return False




def main(config=None):
    """
    Funci√≥n principal con verificaci√≥n y creaci√≥n autom√°tica de tablas
    """
    print("üéµ Iniciando recolecci√≥n COMPLETA de rese√±as de √°lbumes...")
    
    # Si se llama directamente (no desde db_creator)
    if config is None:
        parser = argparse.ArgumentParser(description='Buscar rese√±as de √°lbumes (versi√≥n completa)')
        parser.add_argument('--config', required=True, help='Archivo de configuraci√≥n JSON')
        args = parser.parse_args()
        
        # Cargar configuraci√≥n desde archivo
        try:
            with open(args.config, 'r') as f:
                config_data = json.load(f)
            
            # Combinar configuraciones
            config = {}
            config.update(config_data.get("common", {}))
            config.update(config_data.get("review_scrapper", {}))
        except Exception as e:
            print(f"Error al cargar configuraci√≥n: {e}")
            sys.exit(1)
    
    # Extraer par√°metros de configuraci√≥n
    db_path = config.get('db_path')
    if not db_path:
        print("Error: No se especific√≥ la ruta de la base de datos")
        return
    
    content_service = config.get('content_service', 'five_filters')
    inicio_id = config.get('inicio_id', 0)
    tamano_lote = config.get('tamano_lote', 50)
    pausa_entre_busquedas = config.get('pausa', 2)
    archivo_errores = config.get('archivo_errores')
    include_metacritic = config.get('include_metacritic', True)
    include_aoty = config.get('include_aoty', True)
    force_update = config.get('force_update', False)
    google_api_key = config.get('google_api_key')
    google_cx = config.get('google_cx')
    
    print(f"üìã Configuraci√≥n COMPLETA:")
    print(f"  üìÅ Base de datos: {db_path}")
    print(f"  üîß Servicio de contenido: {content_service}")
    print(f"  üî¢ ID inicial: {inicio_id}")
    print(f"  üì¶ Tama√±o de lote: {tamano_lote}")
    print(f"  ‚è±Ô∏è  Pausa entre b√∫squedas: {pausa_entre_busquedas} segundos")
    print(f"  üéØ Incluir Metacritic: {include_metacritic}")
    print(f"  üèÜ Incluir Album of the Year: {include_aoty}")
    print(f"  üîÑ Force update: {'ACTIVADO' if force_update else 'DESACTIVADO'}")
    
    # Verificar existencia de la base de datos
    if not os.path.exists(db_path):
        print(f"‚ùå Error: La base de datos {db_path} no existe")
        return
    
    # Verificar y crear tablas antes de procesar
    conn, cursor = conectar_db(db_path)
    if not verificar_y_crear_tablas(conn, cursor):
        print("‚ùå Error al verificar/crear tablas. Abortando.")
        conn.close()
        return
    conn.close()
    
    print("‚úÖ Tablas verificadas/creadas correctamente")
    
    # Iniciar procesamiento completo
    try:
        albums_procesados, resenas_encontradas, albums_saltados, metacritic_encontrados, aoty_encontrados, anydecentmusic_encontrados = procesar_albums_con_force_update_y_origen(
            db_path, content_service, include_metacritic, include_aoty, archivo_errores, 
            inicio_id, tamano_lote, pausa_entre_busquedas, force_update,
            google_api_key, google_cx 
        )
        
        print(f"\n{'='*80}")
        print(f"üéâ RESUMEN FINAL COMPLETO")
        print(f"{'='*80}")
        print(f"üìä √Ålbumes procesados: {albums_procesados}")
        print(f"‚è≠Ô∏è  √Ålbumes saltados: {albums_saltados}")
        print(f"üìÑ Rese√±as encontradas: {resenas_encontradas}")
        print(f"üåç AnyDecentMusic: {anydecentmusic_encontrados}")
        if include_metacritic:
            print(f"üéØ Metacritic: {metacritic_encontrados}")
        if include_aoty:
            print(f"üèÜ Album of the Year: {aoty_encontrados}")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"‚ùå Error durante el procesamiento: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()